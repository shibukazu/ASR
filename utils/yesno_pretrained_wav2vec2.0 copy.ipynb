{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- size: 特徴量\n",
    "- length: 時系列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchmetrics.functional import char_error_rate, word_error_rate\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs_int = {\n",
    "    \"dtype\": torch.int32,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "tkwargs_float = {\n",
    "    \"dtype\": torch.float32,\n",
    "    \"device\": \"cuda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoDatasetWav(Dataset):\n",
    "    def __init__(self, wav_dir_path, model_sample_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dataset = []\n",
    "        columns = [\"path\", \"text_idx\"]\n",
    "        self.labels = [\"y\", \"e\", \"s\", \"n\", \"o\", \"<space>\", \"_\"]\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels)}\n",
    "        for wav_file_path in glob.glob(wav_dir_path + \"*.wav\"):\n",
    "            file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "            text_idx = []\n",
    "            for c in file_name:\n",
    "                if c == \"1\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"yes\"] \n",
    "                elif c == \"0\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"no\"] \n",
    "                elif c == \"_\":\n",
    "                    text_idx.append(self.label_to_idx[\"<space>\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid Dir Path\")\n",
    "            dataset.append([wav_file_path, text_idx])\n",
    "        \n",
    "        self.dataset = pd.DataFrame(dataset, columns=columns)\n",
    "        self.model_sample_rate = model_sample_rate\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav_file_path = self.dataset.iloc[idx, 0]\n",
    "        text_idx = self.dataset.iloc[idx, 1]\n",
    "        wav_data, sample_rate = torchaudio.load(wav_file_path)\n",
    "        if sample_rate != self.model_sample_rate:\n",
    "            wav_data = torchaudio.functional.resample(wav_data, sample_rate, self.model_sample_rate)\n",
    "        wav_data = wav_data.squeeze(0)\n",
    "        return wav_data, torch.tensor(text_idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    wavs, text_idxs = zip(*batch)\n",
    "    original_wav_lens = torch.tensor(np.array([len(wav) for wav in wavs]))\n",
    "    original_text_idx_lens = torch.tensor(np.array([len(text_idx) for text_idx in text_idxs]))\n",
    "    # padding for spectrogram_db\n",
    "    padded_wavs = []\n",
    "    for wav in wavs:\n",
    "        padded_wav = np.pad(wav, ((0, max(original_wav_lens)-wav.shape[0])), \"constant\", constant_values=0)\n",
    "        padded_wavs.append(padded_wav)\n",
    "    \n",
    "    padded_wavs = torch.tensor(np.array(padded_wavs))\n",
    "\n",
    "    # padding and packing for text_idx\n",
    "    padded_text_idxs = pad_sequence(text_idxs, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio import pipelines\n",
    "bundle = pipelines.WAV2VEC2_BASE\n",
    "\n",
    "model_sample_rate = bundle.sample_rate\n",
    "wav_dir_path = \"../datasets/waves_yesno/\"\n",
    "dataset = YesNoDatasetWav(wav_dir_path, model_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 学習データとテストデータに分割\n",
    "## 合計サイズが元のサイズと同一になるように注意\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "BATCH_SIZE = 2\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.preprocessing.subsampling import Conv2DSubSampling\n",
    "from modules.transformers.encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nlabel):\n",
    "        super(Model, self).__init__()\n",
    "        self.in_size = bundle._params[\"encoder_embed_dim\"]\n",
    "        self.nlabel = nlabel\n",
    "        self.wav2vec_encoder = bundle.get_model()\n",
    "        self.fc = nn.Linear(self.in_size, self.nlabel, bias=True)\n",
    "        self.log_softmax = nn.functional.log_softmax\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T]\n",
    "        #   x_lengths: [B]\n",
    "        #       padding前のシーケンス長\n",
    "        # return:\n",
    "        #   log_prob: [B, T, nlabel]\n",
    "        #   y_lengths: [B]\n",
    "        #       非パディング部分のシーケンス長\n",
    "        encoded, y_lengths = self.wav2vec_encoder.extract_features(x, x_lengths) # encoded: [L, B, T, in_size]\n",
    "\n",
    "        y = self.fc(encoded[-1]) # [B, T', nlabel]\n",
    "        \n",
    "        log_probs = self.log_softmax(y, dim=2) # [B, T', nlabel]\n",
    "        return log_probs, y_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This learning will be running on cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This learning will be running on {device}.\")\n",
    "\n",
    "num_labels = len(dataset.labels)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降、各モデルごとに実験用のコードを記述していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(hypotheses_idxs, labels, padding_idx):\n",
    "    # hypothesis_idxs: tensor(batch, time)\n",
    "    # labels: np.array(num_labels)\n",
    "\n",
    "    hypotheses_idxs = hypotheses_idxs.cpu().numpy()\n",
    "    hypotheses = []\n",
    "    blank_idx = labels.index(\"_\")\n",
    "    for hypothesis_idxs in hypotheses_idxs:\n",
    "        hypothesis = []\n",
    "        prev_idx = -1\n",
    "        for idx in hypothesis_idxs:\n",
    "            if idx == blank_idx:\n",
    "                continue\n",
    "            elif idx == prev_idx:\n",
    "                continue\n",
    "            elif idx == padding_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if labels[idx] == \"<space>\":\n",
    "                    hypothesis.append(\" \")\n",
    "                else:\n",
    "                    hypothesis.append(labels[idx])\n",
    "                prev_idx = idx\n",
    "        hypotheses.append(\"\".join(hypothesis))\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class TransformerLR(_LRScheduler):\n",
    "    \"\"\"TransformerLR class for adjustment of learning rate.\n",
    "\n",
    "    The scheduling is based on the method proposed in 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_epochs=1000, last_epoch=-1, verbose=False):\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.normalize = self.warmup_epochs**0.5\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Return adjusted learning rate.\"\"\"\n",
    "        step = self.last_epoch + 1\n",
    "        scale = self.normalize * min(step**-0.5, step * self.warmup_epochs**-1.5)\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0 copy.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m Model(num_labels)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ctc_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCTCLoss(reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m, blank\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mlabel_to_idx[\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 570 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    572\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "model = Model(num_labels).to(device)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(reduction=\"sum\", blank=dataset.label_to_idx[\"_\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = TransformerLR(optimizer, warmup_epochs=1000)\n",
    "# Adam\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens) in enumerate(train_dataloader):\n",
    "        cnt += 1\n",
    "        padded_wavs = padded_wavs.to(device)\n",
    "        original_wav_lens = original_wav_lens.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "        original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_probs, y_lengths  = model(x=padded_wavs, x_lengths=original_wav_lens)\n",
    "\n",
    "        loss = ctc_loss(log_probs.transpose(1, 0), padded_text_idxs, y_lengths, original_text_idx_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lossはバッチ内平均ロス\n",
    "        epoch_loss += (loss.item() / BATCH_SIZE)\n",
    "    scheduler.step()\n",
    "    # バッチ内平均ロスの和をイテレーション数で割ることで、一つのデータあたりの平均ロスを求める\n",
    "    writer.add_scalar(\"Loss/Training\", epoch_loss / cnt, i)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = 0\n",
    "        cnt = 0\n",
    "        total_cer = 0\n",
    "        for _, (padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "            cnt += 1\n",
    "            padded_wavs = padded_wavs.to(device)\n",
    "            original_wav_lens = original_wav_lens.to(device)\n",
    "            padded_text_idxs = padded_text_idxs.to(device)\n",
    "            original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "            \n",
    "            log_probs, y_lengths  = model(x=padded_wavs, x_lengths=original_wav_lens)\n",
    "            loss = ctc_loss(log_probs.transpose(1, 0), padded_text_idxs, y_lengths, original_text_idx_lens)\n",
    "            epoch_test_loss += loss.item()\n",
    "            # for CER calculation\n",
    "            hypotheses_idxs = log_probs.argmax(dim=2) \n",
    "            hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "            teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "            total_cer += char_error_rate(hypotheses, teachers)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Test\", epoch_test_loss / cnt, i)\n",
    "    writer.add_scalar(\"CER/Test\", total_cer / cnt, i)\n",
    "    t1 = time.time()\n",
    "    print(f\"{i} epoch: {epoch_loss / cnt} loss, {epoch_test_loss / cnt} test loss, CER: {total_cer / cnt}, {t1 - t0} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp: yes yes no yes yes no no yes\n",
      "tea: yes yes no yes yes no no yes\n",
      "hyp: no yes no no no yes no no\n",
      "tea: no yes no no no yes no no\n",
      "hyp: no yes yes yes yes no yes no\n",
      "tea: no yes yes yes yes no yes no\n",
      "hyp: yes yes yes no yes no yes yes\n",
      "tea: yes yes yes no yes no yes yes\n",
      "hyp: no no yes no no yes yes yes\n",
      "tea: no no yes no no yes yes yes\n",
      "hyp: no yes no yes yes yes no no\n",
      "tea: no yes no yes yes yes no no\n",
      "hyp: no no yes yes no no no yes\n",
      "tea: no no yes yes no no no yes\n",
      "hyp: no no no yes no no no yes\n",
      "tea: no no no yes no no no yes\n",
      "hyp: yes yes no no yes yes yes no\n",
      "tea: yes yes no no yes yes yes no\n",
      "hyp: yes no no no no no no no\n",
      "tea: yes no no no no no no no\n",
      "hyp: yes no yes yes yes yes no yes \n",
      "tea: yes no yes yes yes yes no yes\n",
      "hyp: no no yes yes yes yes no no\n",
      "tea: no no yes yes yes yes no no\n",
      "hyp: no yes no no yes no yes yes\n",
      "tea: no yes no no yes no yes yes\n",
      "hyp: no no yes yes no yes yes yes\n",
      "tea: no no yes yes no yes yes yes\n",
      "hyp: yes yes yes no yes no yes no\n",
      "tea: yes yes yes no yes no yes no\n",
      "hyp: no yes yes no no yes yes yes\n",
      "tea: no yes yes no no yes yes yes\n",
      "hyp: yes yes yes no no yes yes yes\n",
      "tea: yes yes yes no no yes yes yes\n",
      "hyp: no yes yes yes no no nono\n",
      "tea: no yes yes yes no no no no\n",
      "CER: 0.004004328977316618\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_cer = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_spectrogram_dbs,padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(device)\n",
    "        original_spectrofram_db_lens = original_spectrofram_db_lens.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "        original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "        \n",
    "        log_probs, y_lengths  = model(x=padded_spectrogram_dbs, x_lengths=original_spectrofram_db_lens)\n",
    "\n",
    "        hypotheses_idxs = log_probs.argmax(dim=2)\n",
    "        hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "        teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "        for hypothesis, teacher in zip(hypotheses, teachers):\n",
    "            print(f\"hyp: {hypothesis}\")\n",
    "            print(f\"tea: {teacher}\")\n",
    "        total_cer += char_error_rate(hypotheses, teachers)\n",
    "        cnt += 1\n",
    "    print(f\"CER: {total_cer / cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
