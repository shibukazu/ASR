{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- size: 特徴量\n",
    "- length: 時系列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchmetrics.functional import char_error_rate\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs_int = {\n",
    "    \"dtype\": torch.int32,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "tkwargs_float = {\n",
    "    \"dtype\": torch.float32,\n",
    "    \"device\": \"cuda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoDataset(Dataset):\n",
    "    def __init__(self, wav_dir_path, model_sample_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dataset = []\n",
    "        columns = [\"path\", \"text_idx\"]\n",
    "        self.labels = [\"y\", \"e\", \"s\", \"n\", \"o\", \"<space>\", \"_\"]\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels)}\n",
    "        for wav_file_path in glob.glob(wav_dir_path + \"*.wav\"):\n",
    "            file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "            text_idx = []\n",
    "            for c in file_name:\n",
    "                if c == \"1\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"yes\"] \n",
    "                elif c == \"0\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"no\"] \n",
    "                elif c == \"_\":\n",
    "                    text_idx.append(self.label_to_idx[\"<space>\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid Dir Path\")\n",
    "            dataset.append([wav_file_path, text_idx])\n",
    "        \n",
    "        self.dataset = pd.DataFrame(dataset, columns=columns)\n",
    "        self.model_sample_rate = model_sample_rate\n",
    "        self.spectrogram_transformer = torchaudio.transforms.MelSpectrogram(\n",
    "            # スペクトル設定\n",
    "            sample_rate=self.model_sample_rate,\n",
    "            n_fft=1024,\n",
    "            # スペクトログラム設定\n",
    "            win_length= None,\n",
    "            hop_length= 512,\n",
    "            window_fn= torch.hann_window,\n",
    "            # メルスペクトログラム設定\n",
    "            n_mels=40,\n",
    "            power=2.0,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav_file_path = self.dataset.iloc[idx, 0]\n",
    "        text_idx = self.dataset.iloc[idx, 1]\n",
    "        wav_data, sample_rate = torchaudio.load(wav_file_path)\n",
    "        if sample_rate != self.model_sample_rate:\n",
    "            wav_data = torchaudio.functional.resample(wav_data, sample_rate, self.model_sample_rate)\n",
    "            sample_rate = self.model_sample_rate\n",
    "        spectrogram = self.spectrogram_transformer(wav_data)\n",
    "        spectrogram_db = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "        return spectrogram_db[0].transpose(1,0), torch.tensor(text_idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # spectrogram_db: tensor[Time, Melbins]\n",
    "    # text_idx: tensor[text_len]\n",
    "    spectrogram_dbs, text_idxs = zip(*batch)\n",
    "   \n",
    "    original_spectrogram_db_lens = torch.tensor(np.array([len(spectrogram_db) for spectrogram_db in spectrogram_dbs]))\n",
    "    original_text_idx_lens = torch.tensor(np.array([len(text_idx) for text_idx in text_idxs]))\n",
    "\n",
    "    # padding and packing for spectrogram_db\n",
    "    padded_spectrogram_dbs = []\n",
    "    for spectrogram_db in spectrogram_dbs:\n",
    "        padded_spectrogram_db = np.pad(spectrogram_db, ((0,max(original_spectrogram_db_lens)-spectrogram_db.shape[0]),(0,0)), \"constant\", constant_values=0)\n",
    "        padded_spectrogram_dbs.append(padded_spectrogram_db)\n",
    "    \n",
    "    padded_spectrogram_dbs = torch.tensor(np.array(padded_spectrogram_dbs))\n",
    "\n",
    "    # padding and packing for text_idx\n",
    "    padded_text_idxs = pad_sequence(text_idxs, batch_first=True, padding_value=-1)\n",
    "    #packed_padded_texts = pack_padded_sequence(padded_texts, original_text_idx_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    # テキストはCTCロス計算でしか使わず、RNNに入力しないのでpackingによるマスクは不要\n",
    "    return padded_spectrogram_dbs, padded_text_idxs, original_spectrogram_db_lens, original_text_idx_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_rate = 8000\n",
    "wav_dir_path = \"../dataset/waves_yesno/\"\n",
    "dataset = YesNoDataset(wav_dir_path, model_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 学習データとテストデータに分割\n",
    "## 合計サイズが元のサイズと同一になるように注意\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "BATCH_SIZE = 2\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_modules import Conv2DSubSampling, PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, input_size=256, input_size_hidden=2048, nhead=4, dropout=0.1, norm_first=True):\n",
    "        # input_size: 入力の特徴量次元\n",
    "        # input_size_hidden: \n",
    "        #   Linearにおける中間層の次元\n",
    "        #   Attention is all you needにおいてはReLU(x*W_1 + b_1)*W_2 + b_2にて計算される\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        # embed_dimは特徴量次元のサイズであり、vdim, kdimを指定しない場合はembed_dimと同じになる\n",
    "        # 出力は[B, T, F]となるようにそれぞれのattentionが行われるぽい・・・\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = nn.LayerNorm(input_size)\n",
    "        self.norm2 = nn.LayerNorm(input_size)\n",
    "        self.multi_head_self_attn = nn.MultiheadAttention(embed_dim=input_size, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, input_size_hidden)\n",
    "        self.linear2 = nn.Linear(input_size_hidden, input_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # args:\n",
    "        #   x: [B, T, F]\n",
    "        #   attn_msxk: [B*H, T, F](Hはヘッド数) or [T, F]\n",
    "        #       計算したAttentionWeightに対して加算するマスク\n",
    "        #       マスクされたAttentionWeightは無視される\n",
    "        #       オフラインエンコーダーでは一般にシーケンスすべてを見るためNoneでよい\n",
    "        #   key_padding_mask: [B, T]\n",
    "        #       パディングされた部分をTrueにすることで無視するようにする\n",
    "        #       サブサンプリング済みであれば特に指定しなくてよさそう\n",
    "        # return:\n",
    "        #  x: [B, T, F]\n",
    "        if self.norm_first:\n",
    "            x = self.norm1(x)\n",
    "            x = x + self._multi_head_self_attn(x, attn_mask, key_padding_mask)\n",
    "            x = self.norm2(x)\n",
    "            x = x + self._feed_forward(x)\n",
    "        else:\n",
    "            # Attention is all you needにおける実装\n",
    "            x = x + self._multi_head_self_attn(x, attn_mask, key_padding_mask)\n",
    "            x = self.norm1(x)\n",
    "            x = x + self._feed_forward(x)\n",
    "            x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "    def _multi_head_self_attn(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        x = self.multi_head_self_attn(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask, need_weights=False)[0]\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "    def _feed_forward(self, x):\n",
    "        # Attention is all you needではReLU(x*W_1 + b_1)*W_2 + b_2で計算される\n",
    "        # dropout: 各線形層ごとに適用する\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, subsampled_input_size=256, num_layers=12, n_head=4, input_hidden_size=2048, dropout=0.1, norm_first=True) -> None:\n",
    "        # input_size: 入力の特徴量次元\n",
    "        # subsampled_input_size: サブサンプリング後の入力特徴量次元\n",
    "        # num_layers: TransformerEncoderLayerの数\n",
    "        # n_head: MultiHeadAttentionのヘッド数\n",
    "        # input_hidden_size: TransformerEncoderLayerの線形層における中間層の次元\n",
    "        # dropout: Dropoutの割合\n",
    "        # norm_first: LayerNormを先に行うかどうか\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_subsample = Conv2DSubSampling(input_size, subsampled_input_size)\n",
    "        self.positional_encoding = PositionalEncoding(subsampled_input_size)\n",
    "\n",
    "        transformer_encoder_layer = TransformerEncoderLayer(input_size=subsampled_input_size, input_hidden_size=input_hidden_size, nhead=n_head, dropout=dropout, norm_first=norm_first)\n",
    "        self.transformer_encoder = nn.ModuleList(\n",
    "            # オブジェクトの共有を防ぐためにdeepcopyを使う\n",
    "            [copy.deepcopy(transformer_encoder_layer) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm_first = norm_first\n",
    "        if self.norm_first:\n",
    "            self.norm = nn.LayerNorm(subsampled_input_size)\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T, input_size]\n",
    "        #   x_lengths: [B]\n",
    "        # return:\n",
    "        x = self.positional_encoding(x) # [B, T', subsampled_input_size]\n",
    "        key_padding_mask = self._make_key_padding_mask(x_lengths) # [B, T']\n",
    "        for i, layer in enumerate(self.transformer_encoder):\n",
    "            x = layer(x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
    "            if i == self.num_layers // 2:\n",
    "                x_inter = x\n",
    "        if self.norm_first:\n",
    "            x = self.norm(x)\n",
    "            if x_inter is not None:\n",
    "                x_inter = self.norm(x_inter)\n",
    "                \n",
    "        return x, x_inter\n",
    "    \n",
    "    def _create_key_padding_mask(self, x_lengths):\n",
    "        # args:\n",
    "        #   x_lengths: [B]\n",
    "        # return:\n",
    "        #   key_padding_mask: [B, T]\n",
    "        #       パディングされた部分をTrueにすることで無視するようにする\n",
    "        max_len = x_lengths.max()\n",
    "        key_padding_mask = torch.arange(max_len, device=x_lengths.device)[None, :] >= x_lengths[:, None]\n",
    "        return key_padding_mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, subsampled_input_size, num_labels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_labels = num_labels\n",
    "        self.conv2d_sub_sampling = Conv2DSubSampling(input_size, subsampled_input_size, 3, 2, 3, 1)\n",
    "        self.transformer_encoder = TransformerEncoder(subsampled_input_size, 12, 4, 2048, 0.1, True)\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T, input_size]\n",
    "        #   x_lengths: [B]\n",
    "        #       padding前のシーケンス長\n",
    "        subsampled_x, subsampled_x_length = self.conv2d_sub_sampling(x, x_lengths)\n",
    "        key_padding_mask = self._create_key_padding_mask(subsampled_x_length)\n",
    "        output = self.transformer_encoder(subsampled_x, subsampled_x_length) # [B, T', subsampled_input_size]\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降、各モデルごとに実験用のコードを記述していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.nn.functional import log_softmax\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_length, num_labels):\n",
    "        # input_lengthは一つのスペクトルの大きさ\n",
    "        # パディングはバッチ内で数をそろえるため\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.num_labels = num_labels\n",
    "        # time * batch * featureで入力する\n",
    "        #self.rnn = nn.RNN(input_size=self.input_length, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.rnn = nn.LSTM(input_size=self.input_length, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(64, self.num_labels, bias=True)\n",
    "\n",
    "    def forward(self, padded_x, original_x_lens):\n",
    "        # x: device済みのpadded_sequence\n",
    "        packed_padded_x = pack_padded_sequence(padded_x, original_x_lens, batch_first=True, enforce_sorted=False)\n",
    "        packed_padded_x_rnn, hidden = self.rnn(packed_padded_x, None)\n",
    "        # unpackした上で全結合層へ\n",
    "        padded_x_rnn = pad_packed_sequence(packed_padded_x_rnn, batch_first=True, padding_value=0)[0]\n",
    "        padded_y = self.fc(padded_x_rnn)\n",
    "        padded_log_prob = log_softmax(padded_y, dim=2)\n",
    "        \n",
    "        return padded_log_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_length, subsampled_input_length, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.num_labels = num_labels\n",
    "        self.conv2d_sub_sampling = Conv2DSubSampling(input_length, subsampled_input_length, 3, 2, 3, 1)\n",
    "        self.rnn = RNN(subsampled_input_length, self.num_labels)\n",
    "    \n",
    "    def forward(self, padded_spectrogram_dbs):\n",
    "        sub_sampled_padded_spectrogram_dbs = self.conv2d_sub_sampling(padded_spectrogram_dbs)\n",
    "        sub_sampled_padded_spectrogram_db_lens = torch.tensor([sub_sampled_padded_spectrogram_dbs.shape[1] for _ in range(sub_sampled_padded_spectrogram_dbs.shape[0])])\n",
    "        padded_log_prob = self.rnn(sub_sampled_padded_spectrogram_dbs, sub_sampled_padded_spectrogram_db_lens)\n",
    "        return padded_log_prob, sub_sampled_padded_spectrogram_db_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This learning will be running on cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This learning will be running on {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 40\n",
    "subsampled_input_length = 32\n",
    "num_labels = len(dataset.labels)\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(hypotheses_idxs, labels, padding_idx):\n",
    "    # hypothesis_idxs: tensor(batch, time)\n",
    "    # labels: np.array(num_labels)\n",
    "\n",
    "    hypotheses_idxs = hypotheses_idxs.cpu().numpy()\n",
    "    hypotheses = []\n",
    "    blank_idx = labels.index(\"_\")\n",
    "    for hypothesis_idxs in hypotheses_idxs:\n",
    "        hypothesis = []\n",
    "        prev_idx = -1\n",
    "        for idx in hypothesis_idxs:\n",
    "            if idx == blank_idx:\n",
    "                continue\n",
    "            elif idx == prev_idx:\n",
    "                continue\n",
    "            elif idx == padding_idx:\n",
    "                continue\n",
    "            else:\n",
    "                hypothesis.append(labels[idx])\n",
    "                prev_idx = idx\n",
    "        hypotheses.append(\"\".join(hypothesis))\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: 110.57820426093207 loss, 95.21940019395616 test loss, CER: 0.8986272811889648, 0.6719908714294434 sec\n",
      "1 epoch: 108.1518071492513 loss, 94.02442762586806 test loss, CER: 0.9991829991340637, 0.6908049583435059 sec\n",
      "2 epoch: 105.6210560268826 loss, 90.56393517388238 test loss, CER: 0.9048658013343811, 0.6970508098602295 sec\n",
      "3 epoch: 98.65273412068684 loss, 81.75259314643012 test loss, CER: 0.4814228415489197, 0.7050962448120117 sec\n",
      "4 epoch: 81.96647220187717 loss, 59.058189392089844 test loss, CER: 0.15483933687210083, 0.709968090057373 sec\n",
      "5 epoch: 52.83677122328017 loss, 33.4733698103163 test loss, CER: 0.0781707614660263, 0.7209258079528809 sec\n",
      "6 epoch: 30.53088665008545 loss, 20.97452820671929 test loss, CER: 0.05267477035522461, 0.7451949119567871 sec\n",
      "7 epoch: 20.224013063642715 loss, 15.957574844360352 test loss, CER: 0.05020396411418915, 0.7213895320892334 sec\n",
      "8 epoch: 15.447737587822807 loss, 10.982566939459907 test loss, CER: 0.04708058014512062, 0.7611837387084961 sec\n",
      "9 epoch: 10.358913527594673 loss, 8.28316158718533 test loss, CER: 0.0191090926527977, 0.7540154457092285 sec\n",
      "10 epoch: 7.674505286746555 loss, 7.402388201819526 test loss, CER: 0.025630470365285873, 0.7237586975097656 sec\n",
      "11 epoch: 5.969249619377984 loss, 6.1704451243082685 test loss, CER: 0.03035738132894039, 0.7490923404693604 sec\n",
      "12 epoch: 4.680854969554478 loss, 5.3865506119198265 test loss, CER: 0.03035653568804264, 0.7469487190246582 sec\n",
      "13 epoch: 3.696966290473938 loss, 4.891876220703125 test loss, CER: 0.01930917240679264, 0.7709732055664062 sec\n",
      "14 epoch: 2.9327543444103665 loss, 4.289069864484999 test loss, CER: 0.01373470202088356, 0.7454230785369873 sec\n",
      "15 epoch: 2.4198494222429066 loss, 4.0173393090566 test loss, CER: 0.013694154098629951, 0.7368805408477783 sec\n",
      "16 epoch: 2.0012064112557306 loss, 3.9394370714823403 test loss, CER: 0.013762323185801506, 0.724229097366333 sec\n",
      "17 epoch: 1.6969616214434307 loss, 3.822902864880032 test loss, CER: 0.013654185459017754, 0.7155215740203857 sec\n",
      "18 epoch: 1.5236163073115878 loss, 3.7701424757639566 test loss, CER: 0.013705823570489883, 0.7179827690124512 sec\n",
      "19 epoch: 1.3166183001465268 loss, 3.539188782374064 test loss, CER: 0.013702147640287876, 0.7112901210784912 sec\n",
      "20 epoch: 1.153280069430669 loss, 3.708520551522573 test loss, CER: 0.013745435513556004, 0.7356224060058594 sec\n",
      "21 epoch: 1.0268952978981867 loss, 3.7275690502590604 test loss, CER: 0.013775840401649475, 0.7569115161895752 sec\n",
      "22 epoch: 0.9415434797604879 loss, 3.714907421006097 test loss, CER: 0.013713649474084377, 0.7396941184997559 sec\n",
      "23 epoch: 0.8557919032043881 loss, 3.395631856388516 test loss, CER: 0.013641261495649815, 0.7389390468597412 sec\n",
      "24 epoch: 0.7870936128828261 loss, 3.502812922000885 test loss, CER: 0.013694154098629951, 0.7187833786010742 sec\n",
      "25 epoch: 0.7289203074243333 loss, 3.4222623308499656 test loss, CER: 0.013567684218287468, 0.7376930713653564 sec\n",
      "26 epoch: 0.6693732606040107 loss, 3.4209993216726513 test loss, CER: 0.013601291924715042, 0.7330429553985596 sec\n",
      "27 epoch: 0.6243866334358851 loss, 3.602552122539944 test loss, CER: 0.013602975755929947, 0.756446361541748 sec\n",
      "28 epoch: 0.5826893068022199 loss, 3.219852086570528 test loss, CER: 0.013747826218605042, 0.7291595935821533 sec\n",
      "29 epoch: 0.5471036715639962 loss, 3.3672898444864483 test loss, CER: 0.013597897253930569, 0.7503542900085449 sec\n",
      "30 epoch: 0.5140562305847803 loss, 3.3835632138782077 test loss, CER: 0.013652930036187172, 0.7280223369598389 sec\n",
      "31 epoch: 0.48253580927848816 loss, 3.543257213301129 test loss, CER: 0.013717812485992908, 0.736943244934082 sec\n",
      "32 epoch: 0.45599154465728337 loss, 3.572249021795061 test loss, CER: 0.013719527050852776, 0.7291138172149658 sec\n",
      "33 epoch: 0.43251249028576744 loss, 3.42069282134374 test loss, CER: 0.013624372892081738, 0.7232656478881836 sec\n",
      "34 epoch: 0.408600389957428 loss, 3.436338778999117 test loss, CER: 0.013637297786772251, 0.7644577026367188 sec\n",
      "35 epoch: 0.3884924004475276 loss, 3.6205410493744745 test loss, CER: 0.013770130462944508, 0.7307729721069336 sec\n",
      "36 epoch: 0.3704170667462879 loss, 3.538356820742289 test loss, CER: 0.013654185459017754, 0.7257840633392334 sec\n",
      "37 epoch: 0.35299910936090684 loss, 3.3720913198259144 test loss, CER: 0.013694154098629951, 0.7567598819732666 sec\n",
      "38 epoch: 0.33560139271948075 loss, 3.4055850671397314 test loss, CER: 0.013740493915975094, 0.7413811683654785 sec\n",
      "39 epoch: 0.3199791626797782 loss, 3.4749390714698367 test loss, CER: 0.013694154098629951, 0.7598950862884521 sec\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "model = Model(input_length, subsampled_input_length, num_labels).to(device)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(reduction=\"sum\", blank=dataset.label_to_idx[\"_\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# Adam\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_spectrogram_dbs, padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(train_dataloader):\n",
    "        cnt += 1\n",
    "        optimizer.zero_grad()\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "      \n",
    "        padded_log_probs, sub_sampled_padded_spectrogram_db_lens = model(padded_spectrogram_dbs)\n",
    "        loss = ctc_loss(padded_log_probs.transpose(1,0), padded_text_idxs, sub_sampled_padded_spectrogram_db_lens, original_text_idx_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lossはバッチ内平均ロス\n",
    "        epoch_loss += (loss.item() / BATCH_SIZE)\n",
    "    # バッチ内平均ロスの和をイテレーション数で割ることで、一つのデータあたりの平均ロスを求める\n",
    "    writer.add_scalar(\"Loss/Training\", epoch_loss / cnt, i)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = 0\n",
    "        cnt = 0\n",
    "        total_cer = 0\n",
    "        for _, (padded_spectrogram_dbs, padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "            cnt += 1\n",
    "            padded_spectrogram_dbs = padded_spectrogram_dbs.to(device)\n",
    "            padded_text_idxs = padded_text_idxs.to(device)\n",
    "            \n",
    "            padded_log_probs, sub_sampled_padded_spectrogram_db_lens = model(padded_spectrogram_dbs)\n",
    "            loss = ctc_loss(padded_log_probs.transpose(1,0), padded_text_idxs, sub_sampled_padded_spectrogram_db_lens, original_text_idx_lens)\n",
    "            epoch_test_loss += loss.item()\n",
    "            # for CER calculation\n",
    "            hypotheses_idxs = padded_log_probs.argmax(dim=2) \n",
    "            hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "            teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "            total_cer += char_error_rate(hypotheses, teachers)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Test\", epoch_test_loss / cnt, i)\n",
    "    writer.add_scalar(\"CER/Test\", total_cer / cnt, i)\n",
    "    t1 = time.time()\n",
    "    print(f\"{i} epoch: {epoch_loss / cnt} loss, {epoch_test_loss / cnt} test loss, CER: {total_cer / cnt}, {t1 - t0} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp: no<space>no<space>yes<space>yes<space>yes<space>yes<space>no<space>no\n",
      "tea: no<space>no<space>yes<space>yes<space>yes<space>yes<space>no<space>no\n",
      "hyp: no<space>yes<space>yes<space>yes<space>yes<space>no<space>yes<space>no\n",
      "tea: no<space>yes<space>yes<space>yes<space>yes<space>no<space>yes<space>no\n",
      "hyp: no<space>yes<space>no<space>yes<space>yes<space>yes<space>no<space>no\n",
      "tea: no<space>yes<space>no<space>yes<space>yes<space>yes<space>no<space>no\n",
      "hyp: no<space>yes<space>yes<space>yes<space>no<space>no<space>no<space>no\n",
      "tea: no<space>yes<space>yes<space>yes<space>no<space>no<space>no<space>no\n",
      "hyp: yes<space>yes<space>no<space>yes<space>no<space>yes<space>yes<space>no\n",
      "tea: yes<space>yes<space>no<space>yes<space>no<space>yes<space>yes<space>no\n",
      "hyp: yes<space>yes<space>no<space>no<space>no<space>yes<space>yes<space>yes\n",
      "tea: yes<space>yes<space>no<space>no<space>no<space>yes<space>yes<space>yes\n",
      "hyp: yes<space>yes<space>yes<space>no<space>no<space>yes<space>no<space>yes\n",
      "tea: yes<space>yes<space>yes<space>no<space>no<space>yes<space>no<space>yes\n",
      "hyp: yes<space>yes<space>yes<space>no<space>yes<space>no<space>yes<space>yes\n",
      "tea: yes<space>yes<space>yes<space>no<space>yes<space>no<space>yes<space>yes\n",
      "hyp: yes<space>no<space>yes<space>no<space>yes<space>no<space>no<space>ye\n",
      "tea: yes<space>no<space>yes<space>no<space>yes<space>no<space>no<space>yes\n",
      "hyp: no<space>yes<space>no<space>yes<space>no<space>no<space>no\n",
      "tea: no<space>yes<space>no<space>yes<space>no<space>no<space>no<space>no\n",
      "hyp: yes<space>yes<space>yes<space>yes<space>yes<space>yes<space>yes<space>yes\n",
      "tea: yes<space>yes<space>yes<space>yes<space>yes<space>yes<space>yes<space>yes\n",
      "hyp: yes<space>yes<space>no<space>yes<space>yes<space>yes<space>yes<space>no\n",
      "tea: yes<space>yes<space>no<space>yes<space>yes<space>yes<space>yes<space>no\n",
      "hyp: yes<space>yes<space>yes<space>yes<space>yes<space>no<space>no<space>no\n",
      "tea: yes<space>yes<space>yes<space>yes<space>yes<space>no<space>no<space>no\n",
      "hyp: no<space>yes<space>no<space>no<space>yes<space>no<space>yes<space>yes\n",
      "tea: no<space>yes<space>no<space>no<space>yes<space>no<space>yes<space>yes\n",
      "hyp: yes<space>yes<space>no<space>no<space>yes<space>yes<space>yes<space>no\n",
      "tea: yes<space>yes<space>no<space>no<space>yes<space>yes<space>yes<space>no\n",
      "hyp: no<space>no<space>no<space>yes<space>no<space>yes<space>yes<space>no\n",
      "tea: no<space>no<space>no<space>yes<space>no<space>yes<space>yes<space>no\n",
      "hyp: <space>yes<space>yes<space>yes<space>yes<space>no<space>yes<space>no<space>no\n",
      "tea: yes<space>yes<space>yes<space>yes<space>no<space>yes<space>no<space>no\n",
      "hyp: no<space>no<space>yes<space>yes<space>no<space>yes<space>yes<space>no\n",
      "tea: no<space>no<space>yes<space>yes<space>no<space>yes<space>yes<space>no\n",
      "CER: 0.013765458017587662\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_cer = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_spectrogram_dbs,padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "        # unpacked_log_probs: tensor[batch, padded_time, num_labels]\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "        padded_log_probs, sub_sampled_padded_spectrogram_db_lens = model(padded_spectrogram_dbs)\n",
    "        hypotheses_idxs = padded_log_probs.argmax(dim=2) \n",
    "        hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "        teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "        for hypothesis, teacher in zip(hypotheses, teachers):\n",
    "            print(f\"hyp: {hypothesis}\")\n",
    "            print(f\"tea: {teacher}\")\n",
    "        total_cer += char_error_rate(hypotheses, teachers)\n",
    "        cnt += 1\n",
    "    print(f\"CER: {total_cer / cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('asr_trial-VSmJ9s8a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Aug  8 2022, 15:33:42) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dda18e6f9c257e1e507a9da78a27ac69b8a9bc5fb214ed912e4418646a6126e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
