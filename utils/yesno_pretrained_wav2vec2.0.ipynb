{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- size: 特徴量\n",
    "- length: 時系列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchmetrics.functional import char_error_rate, word_error_rate\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs_int = {\n",
    "    \"dtype\": torch.int32,\n",
    "    \"device\": \"cuda\",\n",
    "}\n",
    "tkwargs_float = {\n",
    "    \"dtype\": torch.float32,\n",
    "    \"device\": \"cuda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoDatasetWav(Dataset):\n",
    "    def __init__(self, wav_dir_path, model_sample_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        dataset = []\n",
    "        columns = [\"path\", \"text_idx\"]\n",
    "        self.labels = [\"y\", \"e\", \"s\", \"n\", \"o\", \"<space>\", \"_\"]\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(self.labels)}\n",
    "        for wav_file_path in glob.glob(wav_dir_path + \"*.wav\"):\n",
    "            file_name = os.path.splitext(os.path.basename(wav_file_path))[0]\n",
    "            text_idx = []\n",
    "            for c in file_name:\n",
    "                if c == \"1\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"yes\"] \n",
    "                elif c == \"0\":\n",
    "                    text_idx += [self.label_to_idx[ic] for ic in \"no\"] \n",
    "                elif c == \"_\":\n",
    "                    text_idx.append(self.label_to_idx[\"<space>\"])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid Dir Path\")\n",
    "            dataset.append([wav_file_path, text_idx])\n",
    "        \n",
    "        self.dataset = pd.DataFrame(dataset, columns=columns)\n",
    "        self.model_sample_rate = model_sample_rate\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav_file_path = self.dataset.iloc[idx, 0]\n",
    "        text_idx = self.dataset.iloc[idx, 1]\n",
    "        wav_data, sample_rate = torchaudio.load(wav_file_path)\n",
    "        if sample_rate != self.model_sample_rate:\n",
    "            wav_data = torchaudio.functional.resample(wav_data, sample_rate, self.model_sample_rate)\n",
    "        wav_data = wav_data.squeeze(0)\n",
    "        return wav_data, torch.tensor(text_idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    wavs, text_idxs = zip(*batch)\n",
    "    original_wav_lens = torch.tensor(np.array([len(wav) for wav in wavs]))\n",
    "    original_text_idx_lens = torch.tensor(np.array([len(text_idx) for text_idx in text_idxs]))\n",
    "    # padding for spectrogram_db\n",
    "    padded_wavs = []\n",
    "    for wav in wavs:\n",
    "        padded_wav = np.pad(wav, ((0, max(original_wav_lens)-wav.shape[0])), \"constant\", constant_values=0)\n",
    "        padded_wavs.append(padded_wav)\n",
    "    \n",
    "    padded_wavs = torch.tensor(np.array(padded_wavs))\n",
    "\n",
    "    # padding and packing for text_idx\n",
    "    padded_text_idxs = pad_sequence(text_idxs, batch_first=True, padding_value=-1)\n",
    "\n",
    "    return padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio import pipelines\n",
    "bundle = pipelines.WAV2VEC2_BASE\n",
    "\n",
    "model_sample_rate = bundle.sample_rate\n",
    "wav_dir_path = \"../datasets/waves_yesno/\"\n",
    "dataset = YesNoDatasetWav(wav_dir_path, model_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 学習データとテストデータに分割\n",
    "## 合計サイズが元のサイズと同一になるように注意\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "BATCH_SIZE = 2\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.preprocessing.subsampling import Conv2DSubSampling\n",
    "from modules.transformers.encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nlabel):\n",
    "        super(Model, self).__init__()\n",
    "        self.in_size = bundle._params[\"encoder_embed_dim\"]\n",
    "        self.nlabel = nlabel\n",
    "        self.wav2vec_encoder = bundle.get_model()\n",
    "        self.fc = nn.Linear(self.in_size, self.nlabel, bias=True)\n",
    "        self.log_softmax = nn.functional.log_softmax\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T]\n",
    "        #   x_lengths: [B]\n",
    "        #       padding前のシーケンス長\n",
    "        # return:\n",
    "        #   log_prob: [B, T, nlabel]\n",
    "        #   y_lengths: [B]\n",
    "        #       非パディング部分のシーケンス長\n",
    "        encoded, y_lengths = self.wav2vec_encoder.extract_features(x, x_lengths) # encoded: [L, B, T, in_size]\n",
    "\n",
    "        y = self.fc(encoded[-1]) # [B, T', nlabel]\n",
    "        \n",
    "        log_probs = self.log_softmax(y, dim=2) # [B, T', nlabel]\n",
    "        return log_probs, y_lengths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This learning will be running on cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This learning will be running on {device}.\")\n",
    "\n",
    "num_labels = len(dataset.labels)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降、各モデルごとに実験用のコードを記述していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(hypotheses_idxs, labels, padding_idx):\n",
    "    # hypothesis_idxs: tensor(batch, time)\n",
    "    # labels: np.array(num_labels)\n",
    "\n",
    "    hypotheses_idxs = hypotheses_idxs.cpu().numpy()\n",
    "    hypotheses = []\n",
    "    blank_idx = labels.index(\"_\")\n",
    "    for hypothesis_idxs in hypotheses_idxs:\n",
    "        hypothesis = []\n",
    "        prev_idx = -1\n",
    "        for idx in hypothesis_idxs:\n",
    "            if idx == blank_idx:\n",
    "                continue\n",
    "            elif idx == prev_idx:\n",
    "                continue\n",
    "            elif idx == padding_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if labels[idx] == \"<space>\":\n",
    "                    hypothesis.append(\" \")\n",
    "                else:\n",
    "                    hypothesis.append(labels[idx])\n",
    "                prev_idx = idx\n",
    "        hypotheses.append(\"\".join(hypothesis))\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class TransformerLR(_LRScheduler):\n",
    "    \"\"\"TransformerLR class for adjustment of learning rate.\n",
    "\n",
    "    The scheduling is based on the method proposed in 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_epochs=1000, last_epoch=-1, verbose=False):\n",
    "        \"\"\"Initialize class.\"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.normalize = self.warmup_epochs**0.5\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"Return adjusted learning rate.\"\"\"\n",
    "        step = self.last_epoch + 1\n",
    "        scale = self.normalize * min(step**-0.5, step * self.warmup_epochs**-1.5)\n",
    "        return [base_lr * scale for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m scheduler \u001b[39m=\u001b[39m TransformerLR(optimizer, warmup_epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m wer \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39;49m\u001b[39mwer\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/utils/yesno_pretrained_wav2vec2.0.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/loading.py:702\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, config_name, module_type, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **init_kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[39m\"\"\"Load a `evaluate.EvaluationModule`.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39m    `evaluate.EvaluationModule`\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    701\u001b[0m download_mode \u001b[39m=\u001b[39m DownloadMode(download_mode \u001b[39mor\u001b[39;00m DownloadMode\u001b[39m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m--> 702\u001b[0m evaluation_module \u001b[39m=\u001b[39m evaluation_module_factory(\n\u001b[1;32m    703\u001b[0m     path, module_type\u001b[39m=\u001b[39;49mmodule_type, revision\u001b[39m=\u001b[39;49mrevision, download_config\u001b[39m=\u001b[39;49mdownload_config, download_mode\u001b[39m=\u001b[39;49mdownload_mode\n\u001b[1;32m    704\u001b[0m )\n\u001b[1;32m    705\u001b[0m evaluation_cls \u001b[39m=\u001b[39m import_main_class(evaluation_module\u001b[39m.\u001b[39mmodule_path)\n\u001b[1;32m    706\u001b[0m evaluation_instance \u001b[39m=\u001b[39m evaluation_cls(\n\u001b[1;32m    707\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[1;32m    708\u001b[0m     process_id\u001b[39m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minit_kwargs,\n\u001b[1;32m    715\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/loading.py:618\u001b[0m, in \u001b[0;36mevaluation_module_factory\u001b[0;34m(path, module_type, revision, download_config, download_mode, force_local_path, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mfor\u001b[39;00m current_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomparison\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmeasurement\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    617\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m         \u001b[39mreturn\u001b[39;00m HubEvaluationModuleFactory(\n\u001b[1;32m    619\u001b[0m             \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mevaluate-\u001b[39;49m\u001b[39m{\u001b[39;49;00mcurrent_type\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    620\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    621\u001b[0m             download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    622\u001b[0m             download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m    623\u001b[0m             dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m    624\u001b[0m         )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m    625\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/loading.py:476\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ImportableModule:\n\u001b[1;32m    475\u001b[0m     \u001b[39m# get script and other files\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m     local_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_loading_script()\n\u001b[1;32m    477\u001b[0m     imports \u001b[39m=\u001b[39m get_imports(local_path)\n\u001b[1;32m    478\u001b[0m     local_imports \u001b[39m=\u001b[39m _download_additional_modules(\n\u001b[1;32m    479\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    480\u001b[0m         base_path\u001b[39m=\u001b[39mhf_hub_url(path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, revision\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrevision),\n\u001b[1;32m    481\u001b[0m         imports\u001b[39m=\u001b[39mimports,\n\u001b[1;32m    482\u001b[0m         download_config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_config,\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/loading.py:472\u001b[0m, in \u001b[0;36mHubEvaluationModuleFactory.download_loading_script\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading builder script\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(file_path, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/utils/file_utils.py:224\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    223\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    225\u001b[0m         url_or_filename,\n\u001b[1;32m    226\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    227\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    228\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    229\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    230\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    231\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    232\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    233\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    234\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    235\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    236\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    239\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/utils/file_utils.py:513\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[1;32m    511\u001b[0m     connected \u001b[39m=\u001b[39m ftp_head(url)\n\u001b[1;32m    512\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     response \u001b[39m=\u001b[39m http_head(\n\u001b[1;32m    514\u001b[0m         url,\n\u001b[1;32m    515\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    516\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    517\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m    518\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:  \u001b[39m# ok\u001b[39;00m\n\u001b[1;32m    522\u001b[0m         etag \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m use_etag \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/utils/file_utils.py:427\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    425\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    426\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 427\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    428\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    429\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    430\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    431\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    432\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    433\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    434\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    435\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    436\u001b[0m )\n\u001b[1;32m    437\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/evaluate/utils/file_utils.py:356\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    354\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    355\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    357\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m is_new_proxy_conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproxy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    697\u001b[0m     conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m is_new_proxy_conn \u001b[39mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    711\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/urllib3/connectionpool.py:996\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    994\u001b[0m     conn\u001b[39m.\u001b[39mtls_in_tls_required \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m conn\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "model = Model(num_labels).to(device)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(reduction=\"sum\", blank=dataset.label_to_idx[\"_\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = TransformerLR(optimizer, warmup_epochs=1000)\n",
    "# Adam\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens) in enumerate(train_dataloader):\n",
    "        cnt += 1\n",
    "        padded_wavs = padded_wavs.to(device)\n",
    "        original_wav_lens = original_wav_lens.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "        original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_probs, y_lengths  = model(x=padded_wavs, x_lengths=original_wav_lens)\n",
    "\n",
    "        loss = ctc_loss(log_probs.transpose(1, 0), padded_text_idxs, y_lengths, original_text_idx_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lossはバッチ内平均ロス\n",
    "        epoch_loss += (loss.item() / BATCH_SIZE)\n",
    "    scheduler.step()\n",
    "    # バッチ内平均ロスの和をイテレーション数で割ることで、一つのデータあたりの平均ロスを求める\n",
    "    writer.add_scalar(\"Loss/Training\", epoch_loss / cnt, i)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = 0\n",
    "        cnt = 0\n",
    "        total_cer = 0\n",
    "        for _, (padded_wavs, padded_text_idxs, original_wav_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "            cnt += 1\n",
    "            padded_wavs = padded_wavs.to(device)\n",
    "            original_wav_lens = original_wav_lens.to(device)\n",
    "            padded_text_idxs = padded_text_idxs.to(device)\n",
    "            original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "            \n",
    "            log_probs, y_lengths  = model(x=padded_wavs, x_lengths=original_wav_lens)\n",
    "            loss = ctc_loss(log_probs.transpose(1, 0), padded_text_idxs, y_lengths, original_text_idx_lens)\n",
    "            epoch_test_loss += loss.item()\n",
    "            # for CER calculation\n",
    "            hypotheses_idxs = log_probs.argmax(dim=2) \n",
    "            hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "            teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "            total_cer += char_error_rate(hypotheses, teachers)\n",
    "\n",
    "    writer.add_scalar(\"Loss/Test\", epoch_test_loss / cnt, i)\n",
    "    writer.add_scalar(\"CER/Test\", total_cer / cnt, i)\n",
    "    t1 = time.time()\n",
    "    print(f\"{i} epoch: {epoch_loss / cnt} loss, {epoch_test_loss / cnt} test loss, CER: {total_cer / cnt}, {t1 - t0} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp: yes yes no yes yes no no yes\n",
      "tea: yes yes no yes yes no no yes\n",
      "hyp: no yes no no no yes no no\n",
      "tea: no yes no no no yes no no\n",
      "hyp: no yes yes yes yes no yes no\n",
      "tea: no yes yes yes yes no yes no\n",
      "hyp: yes yes yes no yes no yes yes\n",
      "tea: yes yes yes no yes no yes yes\n",
      "hyp: no no yes no no yes yes yes\n",
      "tea: no no yes no no yes yes yes\n",
      "hyp: no yes no yes yes yes no no\n",
      "tea: no yes no yes yes yes no no\n",
      "hyp: no no yes yes no no no yes\n",
      "tea: no no yes yes no no no yes\n",
      "hyp: no no no yes no no no yes\n",
      "tea: no no no yes no no no yes\n",
      "hyp: yes yes no no yes yes yes no\n",
      "tea: yes yes no no yes yes yes no\n",
      "hyp: yes no no no no no no no\n",
      "tea: yes no no no no no no no\n",
      "hyp: yes no yes yes yes yes no yes \n",
      "tea: yes no yes yes yes yes no yes\n",
      "hyp: no no yes yes yes yes no no\n",
      "tea: no no yes yes yes yes no no\n",
      "hyp: no yes no no yes no yes yes\n",
      "tea: no yes no no yes no yes yes\n",
      "hyp: no no yes yes no yes yes yes\n",
      "tea: no no yes yes no yes yes yes\n",
      "hyp: yes yes yes no yes no yes no\n",
      "tea: yes yes yes no yes no yes no\n",
      "hyp: no yes yes no no yes yes yes\n",
      "tea: no yes yes no no yes yes yes\n",
      "hyp: yes yes yes no no yes yes yes\n",
      "tea: yes yes yes no no yes yes yes\n",
      "hyp: no yes yes yes no no nono\n",
      "tea: no yes yes yes no no no no\n",
      "CER: 0.004004328977316618\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_cer = 0\n",
    "    cnt = 0\n",
    "    for _, (padded_spectrogram_dbs,padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(test_dataloader):\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(device)\n",
    "        original_spectrofram_db_lens = original_spectrofram_db_lens.to(device)\n",
    "        padded_text_idxs = padded_text_idxs.to(device)\n",
    "        original_text_idx_lens = original_text_idx_lens.to(device)\n",
    "        \n",
    "        log_probs, y_lengths  = model(x=padded_spectrogram_dbs, x_lengths=original_spectrofram_db_lens)\n",
    "\n",
    "        hypotheses_idxs = log_probs.argmax(dim=2)\n",
    "        hypotheses = ctc_simple_decode(hypotheses_idxs, dataset.labels, -1)\n",
    "        teachers = ctc_simple_decode(padded_text_idxs, dataset.labels, -1)\n",
    "        for hypothesis, teacher in zip(hypotheses, teachers):\n",
    "            print(f\"hyp: {hypothesis}\")\n",
    "            print(f\"tea: {teacher}\")\n",
    "        total_cer += char_error_rate(hypotheses, teachers)\n",
    "        cnt += 1\n",
    "    print(f\"CER: {total_cer / cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
