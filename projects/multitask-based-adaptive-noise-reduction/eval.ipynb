{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CausalConformerVADAdapterCTCModel' from 'ctc_model' (/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/ctc_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/eval.ipynb Cell 2\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsacs11/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctc_model\u001b[39;00m \u001b[39mimport\u001b[39;00m CausalConformerVADAdapterCTCModel\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs11/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs11/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m DEVICE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CausalConformerVADAdapterCTCModel' from 'ctc_model' (/n/work3/shibutani/ASR/projects/multitask-based-adaptive-noise-reduction/ctc_model.py)"
     ]
    }
   ],
   "source": [
    "from ctc_model import CausalConformerVADAdapterCTCModel\n",
    "import torch\n",
    "DEVICE = \"cuda\"\n",
    "model_path = \"./artifacts/ctc_vad_adapter_csj_pretrain/d0c22404e4aa4653aa3685a44b710e93/artifacts/model_20.pth\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    cpt = torch.load(f)\n",
    "model_state = cpt[\"model\"]\n",
    "model_args = cpt[\"model_args\"]\n",
    "model = CausalConformerVADAdapterCTCModel(**model_args).to(DEVICE)\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import SentencePieceTokenizer\n",
    "\n",
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_file_path=\"./vocabs/csj_train_nodup_sp_4096.bpe.model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CSJVADPretrainDataset, CSJVADAdaptationDataset\n",
    "\"\"\"\n",
    "dev_dataset = CSJVADPretrainDataset(\n",
    "    json_file_path=\"./json/aligned_csj/noisy_pretrain_eval_with_subsampled_vad.json\",\n",
    "    resampling_rate=16000,\n",
    "    tokenizer=tokenizer,\n",
    "    spec_aug=None,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "dev_dataset =CSJVADAdaptationDataset(\n",
    "    json_file_path=\"./json/aligned_csj/noisy_adaptation_with_subsampled_vad.json\",\n",
    "    resampling_rate=16000,\n",
    "    tokenizer=tokenizer,\n",
    "    spec_aug=None,\n",
    ")\n",
    "\n",
    "idx = 3\n",
    "_, x, y, x_len, y_len, _, _, _ = dev_dataset[idx]\n",
    "bx = x.unsqueeze(0).to(DEVICE)\n",
    "by = y.unsqueeze(0).to(DEVICE)\n",
    "bx_len = x_len.unsqueeze(0).to(DEVICE)\n",
    "by_len = y_len.unsqueeze(0).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import char_error_rate\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    bx = bx.to(DEVICE)\n",
    "    bhyp_token_ids = model.streaming_greedy_inference(bx, bx_len, 500)\n",
    "    #bhyp_token_ids = model.greedy_inference(bx, bx_len)\n",
    "bans_token_ids = [by[i, : by_len[i]].tolist() for i in range(by.shape[0])]\n",
    "bhyp_text = tokenizer.batch_token_ids_to_text(bhyp_token_ids)\n",
    "bans_text = tokenizer.batch_token_ids_to_text(bans_token_ids)\n",
    "for i in range(len(bhyp_text)):\n",
    "    print(bhyp_text[i])\n",
    "    print(bans_text[i])\n",
    "    print()\n",
    "cer = char_error_rate(bhyp_text, bans_text)\n",
    "print(cer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
