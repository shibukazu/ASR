{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "import copy\n",
    "import json\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この二つは実験によっては変えるべき\n",
    "self.tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    ")\n",
    "self.feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchaudio.functional import resample\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "class LJSpeech(Dataset):\n",
    "    def __init__(self, dataset_pkl_path: str, sample_rate: int = 16000):\n",
    "        # Paremeter\n",
    "        self.feature_max_length = None\n",
    "        self.label_max_length = None\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        if not os.path.isfile(dataset_pkl_path):\n",
    "            dataset = load_dataset(\"../../datasets/loading_scripts/lj_speech.py\", data_dir=\"../../datasets/LJSpeech-1.1/\")\n",
    "            dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "            dataset = dataset.remove_columns([\"id\"])\n",
    "\n",
    "            self.extract_vocab(train_all_texts=dataset[\"train\"][\"text\"], vocab_path=\"./vocab.json\")\n",
    "            with open(\"vocab.json\", \"r\") as vocab_file:\n",
    "                vocab = json.load(vocab_file)\n",
    "            \n",
    "            def prepare_dataset(batch):\n",
    "                audio = batch[\"audio\"]\n",
    "                batch[\"input_values\"] = resample(audio, orig_freq=audio[\"sampling_rate\"], new_freq=self.sample_rate)\n",
    "                batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "                batch[\"labels\"] = self.tokenizer(batch[\"text\"]).input_ids\n",
    "                return batch\n",
    "\n",
    "            dataset = dataset.map(\n",
    "                prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4\n",
    "            )\n",
    "            with open(dataset_pkl_path, \"wb\") as f:\n",
    "                pickle.dump(dataset, f)\n",
    "        \n",
    "        with open(dataset_pkl_path, \"rb\") as f:\n",
    "            self.dataset = pickle.load(f)\n",
    "        \n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            \"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\"\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"train\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[\"train\"][idx]\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        input_features = [\n",
    "            {\"input_values\": feature[\"input_values\"]} for feature in batch\n",
    "        ]\n",
    "        labels = [\n",
    "            {\"input_ids\": feature[\"labels\"]} for feature in batch\n",
    "        ]\n",
    "        input_feature_lenghts = torch.tensor([\n",
    "            feature[\"input_length\"] for feature in batch\n",
    "        ])\n",
    "        \n",
    "        labels = self.tokenizer.pad(\n",
    "            labels,\n",
    "            padding=True,\n",
    "            max_length=self.label_max_length,\n",
    "            pad_to_multiple_of=1,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_features = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=True,\n",
    "            max_length=self.feature_max_length,\n",
    "            pad_to_multiple_of=1,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels = labels[\"input_ids\"].masked_fill(\n",
    "            labels.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        input_features = input_features[\"input_values\"]\n",
    "        \n",
    "        return input_features, labels, input_feature_lenghts\n",
    "\n",
    "    def extract_vocab(\n",
    "        self,\n",
    "        train_all_texts: List = None, \n",
    "        test_all_texts : List = None,\n",
    "        vocab_path: str = \"./vocab.json\",\n",
    "        ) -> None:\n",
    "        if train_all_texts is None:\n",
    "            train_all_texts = []\n",
    "        if test_all_texts is None:\n",
    "            test_all_texts = []\n",
    "\n",
    "        all_text = \" \".join(train_all_texts + test_all_texts)\n",
    "        vocab_list = list(set(all_text))\n",
    "\n",
    "        vocab = {v: k for k, v in enumerate(vocab_list)}\n",
    "        # use | as delimeter in stead of \" \"\n",
    "        vocab[\"|\"] = vocab[\" \"]\n",
    "        # dekete unused char\n",
    "        del vocab[\" \"]\n",
    "        # add unk and pad token\n",
    "        vocab[\"[UNK]\"] = len(vocab)\n",
    "        vocab[\"[PAD]\"] = len(vocab)\n",
    "\n",
    "        with open(vocab_path, \"w\") as vocab_file:\n",
    "            json.dump(vocab, vocab_file)\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = LJSpeech(dataset_pkl_path=\"./ljspeech_dataset.pkl\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    # 不完全なバッチの無視\n",
    "    drop_last=True,\n",
    "    # 高速化?\n",
    "    pin_memory=True,\n",
    "    collate_fn=train_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_102027/4021298803.py\", line 68, in collate_fn\n    input_features = self.feature_extractor.pad(\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 83, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shibutani/fs/ASR/projects/subsetselection-code-iclr/module_trial.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/projects/subsetselection-code-iclr/module_trial.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tmp \u001b[39m=\u001b[39m train_dataloader\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsacs01/home/shibutani/fs/ASR/projects/subsetselection-code-iclr/module_trial.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m features, labels, feature_lengths \u001b[39m=\u001b[39m tmp\u001b[39m.\u001b[39;49m\u001b[39m__next__\u001b[39;49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1202\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1229\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1230\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/_utils.py:434\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 434\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_102027/4021298803.py\", line 68, in collate_fn\n    input_features = self.feature_extractor.pad(\n  File \"/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataset.py\", line 83, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "tmp = train_dataloader.__iter__()\n",
    "features, labels, feature_lengths = tmp.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.pipelines import WAV2VEC2_ASR_BASE_960H\n",
    "model = WAV2VEC2_ASR_BASE_960H.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('|',\n",
       " 'E',\n",
       " 'T',\n",
       " 'A',\n",
       " 'O',\n",
       " 'N',\n",
       " 'I',\n",
       " 'H',\n",
       " 'S',\n",
       " 'R',\n",
       " 'D',\n",
       " 'L',\n",
       " 'U',\n",
       " 'M',\n",
       " 'W',\n",
       " 'C',\n",
       " 'F',\n",
       " 'G',\n",
       " 'Y',\n",
       " 'P',\n",
       " 'B',\n",
       " 'V',\n",
       " 'K',\n",
       " \"'\",\n",
       " 'X',\n",
       " 'J',\n",
       " 'Q',\n",
       " 'Z')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WAV2VEC2_ASR_BASE_960H._labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
