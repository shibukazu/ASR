hydra:
  run:
    dir: /home/shibutani/fs/ASR/projects/rnn-t-trial/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

dataset:
  name: "Librispeech"
  train:
    json_file_path: "./json/librispeech_train-all-960.json"
  dev:
    json_file_path: "./json/librispeech_dev-other.json"

tokenizer:
  model_file_path: "./vocabs/librispeech_1024_bpe.model"

model:
  name: "CausalConformer"
  encoder:
    input_size: 80
    subsampled_input_size: 144
    num_conformer_blocks: 16
    ff_hidden_size: 144
    conv_hidden_size: 144
    conv_kernel_size: 16 # non-causalの半分
    mha_num_heads: 4
    dropout: 0.1
    subsampling_kernel_size1: 3
    subsampling_stride1: 2
    subsampling_kernel_size2: 3
    subsampling_stride2: 2
    num_previous_frames: "all"
  predictor:
    embedding_size: 320
    hidden_size: 320
    num_layers: 1
  jointnet:
    hidden_size: 256
  spec_aug:
    num_freq_mask: 2
    freq_mask_max_length: 30
    num_time_mask: 2
    time_mask_max_length: 40
  fastemit_lambda: 0.004
  warp_rnnt: true
  is_timewise_ln: true

decoder:
  type: "non_streaming_greedy"
  buffer_size: 4

do_decode: true

train:
  num_epoch: 70
  batch_sec: 150
  accum_sec: 3000
  optimize:
    max_grad_norm: 5.0
    lr: 5.0
    warmup_steps: 10000
    weight_decay: 0.000001
    eps: 0.000000001
    beta1: 0.9
    beta2: 0.98