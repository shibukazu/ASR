hydra:
  run:
    dir: /home/shibutani/fs/ASR/projects/rnn-t-trial/outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

dataset:
  name: "Librispeech"
  train:
    json_file_path: "./json/librispeech_train-clean-100.json"
  dev:
    json_file_path: "./json/librispeech_dev-clean.json"

tokenizer:
  model_file_path: "./vocabs/librispeech_char.model"

model:
  name: "CausalConformer"
  encoder:
    input_size: 80
    subsampled_input_size: 256
    num_conformer_blocks: 16
    ff_hidden_size: 1024
    conv_hidden_size: 144
    conv_kernel_size: 15 # non-causalの半分
    mha_num_heads: 4
    dropout: 0.1
    subsampling_kernel_size1: 3
    subsampling_stride1: 2
    subsampling_kernel_size2: 3
    subsampling_stride2: 2
    num_previous_frames: "all"
  predictor:
    embedding_size: 300
    hidden_size: 300
    num_layers: 1
  jointnet:
    hidden_size: 300
  spec_aug:
    num_freq_mask: 2
    freq_mask_max_length: 30
    num_time_mask: 2
    time_mask_max_length: 40

decoder:
  type: "streaming_greedy"
  buffer_size: 4

do_decode: false

train:
  num_epoch: 100
  batch_sec: 200
  accum_sec: 400
  optimize:
    max_grad_norm: 5.0
    lr: 5.0
    warmup_steps: 25000
    weight_decay: 0.001
    eps: 0.000000001
    beta1: 0.9
    beta2: 0.98