{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "import torch.nn\n",
    "from data import YesNoDataset, LibriSpeechDataset, get_dataloader, LibriSpeechTextDataset, get_text_dataloader\n",
    "from model import CausalConformerModel, TorchAudioConformerModel\n",
    "from torchaudio.functional import rnnt_loss\n",
    "from torch.nn.functional import log_softmax\n",
    "from torchmetrics.functional import char_error_rate, word_error_rate\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tokenizer import SentencePieceTokenizer\n",
    "from create_json import create_librispeech_json\n",
    "from modules import torchaudio_conformer\n",
    "from modules.conformer.normalization import CausalLayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40418261/40418261 [00:31<00:00, 1303168.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "json_dict = {}\n",
    "with open(\"./datasets/librispeech/librispeech-lm-norm.txt\", \"r\") as f:\n",
    "    texts = []\n",
    "    # get num of raws\n",
    "    counter = 0\n",
    "    for line in tqdm(f.readlines()):\n",
    "        # remove \\n\n",
    "        line = line[:-1]\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        json_dict[counter] = {\n",
    "            \"raw_transcript\": line,\n",
    "        }\n",
    "        counter += 1\n",
    "with open(\"./json/librispeech_text.json\", \"w\") as f:\n",
    "    json.dump(json_dict, f, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSentencePieceTokenizer.create_model(\\n    transcription_file_path=\"vocabs/librispeech_train_960h_transcripts.txt\",\\n    model_prefix=\"librispeech_char\",\\n    num_tokens=1024,\\n    model_type=\"char\",\\n    character_coverage=1.0,\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SentencePieceTokenizer.create_model(\n",
    "    transcription_file_path=\"vocabs/librispeech_train_960h_transcripts.txt\",\n",
    "    model_prefix=\"librispeech_char\",\n",
    "    num_tokens=1024,\n",
    "    model_type=\"char\",\n",
    "    character_coverage=1.0,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_file_path=\"./vocabs/librispeech_1024_bpe.model\"\n",
    ")\n",
    "dataset = LibriSpeechTextDataset(\n",
    "    json_file_path=\"./json/librispeech_text.json\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dataloader = get_text_dataloader(\n",
    "    dataset,\n",
    "    batch_text_len=1000,\n",
    "    num_workers=8,\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Prepare: 100%|██████████| 28539/28539 [00:00<00:00, 380216.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_file_path=\"./vocabs/librispeech_1024_bpe.model\"\n",
    ")\n",
    "dataset = LibriSpeechDataset(\n",
    "    resampling_rate=16000,\n",
    "    tokenizer=tokenizer,\n",
    "    json_file_path=\"./json/librispeech_train-clean-100.json\",\n",
    ")\n",
    "dataloader = get_dataloader(\n",
    "    dataset,\n",
    "    batch_sec=60,\n",
    "    num_workers=8,\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "iterator = iter(dataloader)\n",
    "for _ in range(30):\n",
    "    next(iterator)\n",
    "_, benc_input, bpred_input, benc_input_length, bpred_input_length, baudio_sec = next(iterator)\n",
    "benc_input = benc_input.to(DEVICE)\n",
    "bpred_input = bpred_input.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60, 74, 14, 67], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpred_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 106,  886,   38,  320,  871,  151,    8,  663,   55,  106,  886,   38,\n",
       "          320,  871,  422,  151,    8,   88,   13,   78,   18,   17,  267,  145,\n",
       "         1005,  191, 1009,   43,  327,   87,    7,  344,  209,   25,  225,  156,\n",
       "           28,    8,  659, 1004,  755,  179,   92, 1001,  999,  580,    8,   15,\n",
       "          331, 1014,   25,   34,  968,   82,  140,  750,   27,   93, 1004,    3,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4],\n",
       "        [ 179,  925,   76,   24,  361,   34,  910,  125,   31,   26,  368,  758,\n",
       "          741,  154,  919,   24,   27,  141,   58,  170, 1012,  119,   25,  277,\n",
       "         1016,   21,  122,  138,   17,  595,   93,   10,  147,  271,   62,   39,\n",
       "            8,  488,   25,  127,  443,  505,  198,   39, 1001,  965,  308,   39,\n",
       "          198,  303,  710,  938,   51,  589,   62,   22,  980,   76,  553,  996,\n",
       "          164,   98,  825,  812,   61,  128,  160,    8,  636,  125,  406,   67,\n",
       "           38,    3],\n",
       "        [   7, 1003,  534,  165,  534,  165,  255,  320,   85,  998,  175,  154,\n",
       "          280,    3,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            4,    4],\n",
       "        [  67,    8,   11,  164,  304,  186,  833,    8,   11,  220, 1011,  100,\n",
       "         1004,  193,  945,  745,   79,  128,  193,  304,   60,  119,   87,  107,\n",
       "           84,  821,  901,   81,  160,   31,   78,  673,  431,  107,   75,   84,\n",
       "          855,  135,    8,   15,   89,   81,  345,    8,  265,   72,  212,   96,\n",
       "          718,   62,   11,  652,  689,    8,  116, 1009, 1009,   17,  112, 1007,\n",
       "           66,  313,   63,  269,  477,   24,    3,    4,    4,    4,    4,    4,\n",
       "            4,    4]], device='cuda:4', dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpred_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([272])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpred_input.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "torch.nn.functional.pad(a, (1, 0), value=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./artifacts/librispeech_clean-100/1a8150c0e8f64726b4f7058ff8fc6c74/artifacts/model_40.pth\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    cpt = torch.load(f)\n",
    "model_state = cpt[\"model\"]\n",
    "model_args = cpt[\"model_args\"]\n",
    "model = CausalConformerModel(**model_args).to(DEVICE)\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:20, 16.09s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    bpadded_output, bpadded_ctc_log_probs, bsubsampled_enc_input_length = model(\n",
    "        padded_enc_input=benc_input,\n",
    "        enc_input_lengths=benc_input_length,\n",
    "        padded_pred_input=bpred_input,\n",
    "        pred_input_lengths=bpred_input_length,\n",
    "    )\n",
    "    loss = rnnt_loss(\n",
    "        logits=bpadded_output,\n",
    "        targets=bpred_input,\n",
    "        logit_lengths=bsubsampled_enc_input_length.to(DEVICE),\n",
    "        target_lengths=bpred_input_length.to(DEVICE),\n",
    "        blank=tokenizer.blank_token_id,\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "    bhyp_token_indices = model.streaming_greedy_inference(\n",
    "        enc_inputs=benc_input, enc_input_lengths=benc_input_length\n",
    "    )\n",
    "    bhyp_text = tokenizer.batch_token_ids_to_text(bhyp_token_indices)\n",
    "    bans_token_indices = [\n",
    "        bpred_input[i, : bpred_input_length[i]].tolist() for i in range(bpred_input.shape[0])\n",
    "    ]\n",
    "    bhyp_text = tokenizer.batch_token_ids_to_text(bhyp_token_indices)\n",
    "    bans_text = tokenizer.batch_token_ids_to_text(bans_token_indices)\n",
    "    \n",
    "    cer = char_error_rate(bhyp_text, bans_text) * benc_input.shape[0]\n",
    "    wer = word_error_rate(bhyp_text, bans_text) * benc_input.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0658)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[605,\n",
       "  19,\n",
       "  209,\n",
       "  646,\n",
       "  356,\n",
       "  119,\n",
       "  8,\n",
       "  904,\n",
       "  604,\n",
       "  25,\n",
       "  106,\n",
       "  886,\n",
       "  24,\n",
       "  111,\n",
       "  87,\n",
       "  7,\n",
       "  369,\n",
       "  10,\n",
       "  356,\n",
       "  78,\n",
       "  304,\n",
       "  59,\n",
       "  106,\n",
       "  886,\n",
       "  269,\n",
       "  150,\n",
       "  35,\n",
       "  87,\n",
       "  7,\n",
       "  10,\n",
       "  559,\n",
       "  28,\n",
       "  416,\n",
       "  69,\n",
       "  19,\n",
       "  31,\n",
       "  169,\n",
       "  573,\n",
       "  1014],\n",
       " [8,\n",
       "  16,\n",
       "  54,\n",
       "  63,\n",
       "  5,\n",
       "  42,\n",
       "  119,\n",
       "  107,\n",
       "  75,\n",
       "  84,\n",
       "  260,\n",
       "  38,\n",
       "  25,\n",
       "  361,\n",
       "  398,\n",
       "  63,\n",
       "  753,\n",
       "  927,\n",
       "  341,\n",
       "  218,\n",
       "  112,\n",
       "  61,\n",
       "  58,\n",
       "  246,\n",
       "  50,\n",
       "  43,\n",
       "  455,\n",
       "  48,\n",
       "  592,\n",
       "  149,\n",
       "  27,\n",
       "  22,\n",
       "  186,\n",
       "  26,\n",
       "  58,\n",
       "  27,\n",
       "  8,\n",
       "  273,\n",
       "  203,\n",
       "  25,\n",
       "  376,\n",
       "  631,\n",
       "  8,\n",
       "  92,\n",
       "  998,\n",
       "  42,\n",
       "  120,\n",
       "  56,\n",
       "  88,\n",
       "  670,\n",
       "  1004,\n",
       "  151,\n",
       "  293,\n",
       "  55,\n",
       "  32,\n",
       "  58,\n",
       "  467,\n",
       "  19,\n",
       "  95,\n",
       "  382,\n",
       "  199,\n",
       "  414,\n",
       "  997,\n",
       "  1016,\n",
       "  213,\n",
       "  998,\n",
       "  1004,\n",
       "  384,\n",
       "  628,\n",
       "  24,\n",
       "  266,\n",
       "  699,\n",
       "  572,\n",
       "  28,\n",
       "  8,\n",
       "  86,\n",
       "  102,\n",
       "  1012,\n",
       "  86,\n",
       "  64,\n",
       "  153],\n",
       " [674,\n",
       "  422,\n",
       "  108,\n",
       "  187,\n",
       "  62,\n",
       "  25,\n",
       "  262,\n",
       "  1021,\n",
       "  1000,\n",
       "  1014,\n",
       "  24,\n",
       "  350,\n",
       "  31,\n",
       "  9,\n",
       "  700,\n",
       "  323,\n",
       "  144,\n",
       "  81,\n",
       "  95,\n",
       "  7,\n",
       "  369,\n",
       "  53,\n",
       "  42,\n",
       "  1001,\n",
       "  312,\n",
       "  31,\n",
       "  220,\n",
       "  1013,\n",
       "  32],\n",
       " [145,\n",
       "  461,\n",
       "  32,\n",
       "  490,\n",
       "  64,\n",
       "  69,\n",
       "  337,\n",
       "  19,\n",
       "  155,\n",
       "  55,\n",
       "  27,\n",
       "  315,\n",
       "  6,\n",
       "  42,\n",
       "  718,\n",
       "  28,\n",
       "  395,\n",
       "  107,\n",
       "  1010,\n",
       "  652,\n",
       "  1004,\n",
       "  19,\n",
       "  196,\n",
       "  42,\n",
       "  878,\n",
       "  61,\n",
       "  10,\n",
       "  42,\n",
       "  999,\n",
       "  86,\n",
       "  80,\n",
       "  95,\n",
       "  45,\n",
       "  100,\n",
       "  24,\n",
       "  92,\n",
       "  359,\n",
       "  51,\n",
       "  79,\n",
       "  494,\n",
       "  1003,\n",
       "  1001,\n",
       "  1001,\n",
       "  1014,\n",
       "  235,\n",
       "  313,\n",
       "  25,\n",
       "  21,\n",
       "  997,\n",
       "  335,\n",
       "  62,\n",
       "  230,\n",
       "  388,\n",
       "  1019,\n",
       "  1004,\n",
       "  71,\n",
       "  140,\n",
       "  60,\n",
       "  161,\n",
       "  63,\n",
       "  862,\n",
       "  107,\n",
       "  24,\n",
       "  1004,\n",
       "  39,\n",
       "  58,\n",
       "  25,\n",
       "  71,\n",
       "  298,\n",
       "  862,\n",
       "  114,\n",
       "  428,\n",
       "  128,\n",
       "  794,\n",
       "  582]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhyp_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-26.7156, -26.0835, -25.7718, -28.0198, -25.3701, -27.5337, -23.8810,\n",
       "        -24.2610, -26.7484, -29.9810, -24.0653, -24.8224, -26.5370, -32.6300,\n",
       "        -32.5643, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628, -12.9628,\n",
       "        -12.9628, -12.9628, -12.9628, -12.9628], device='cuda:4')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpadded_output[0][68, :, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[103, 19, 407, 19, 209, 402, 208, 465, 94, 27, 141, 58, 327],\n",
       " [35,\n",
       "  209,\n",
       "  314,\n",
       "  158,\n",
       "  145,\n",
       "  656,\n",
       "  58,\n",
       "  96,\n",
       "  230,\n",
       "  388,\n",
       "  129,\n",
       "  534,\n",
       "  444,\n",
       "  281,\n",
       "  94,\n",
       "  184,\n",
       "  206,\n",
       "  67,\n",
       "  8,\n",
       "  672,\n",
       "  34,\n",
       "  447,\n",
       "  128,\n",
       "  95,\n",
       "  31,\n",
       "  790,\n",
       "  1004,\n",
       "  79,\n",
       "  94,\n",
       "  141,\n",
       "  32,\n",
       "  112,\n",
       "  103,\n",
       "  128,\n",
       "  582,\n",
       "  25,\n",
       "  155,\n",
       "  26,\n",
       "  17,\n",
       "  212,\n",
       "  8,\n",
       "  322,\n",
       "  814,\n",
       "  145,\n",
       "  989],\n",
       " [593,\n",
       "  519,\n",
       "  27,\n",
       "  694,\n",
       "  27,\n",
       "  129,\n",
       "  975,\n",
       "  313,\n",
       "  338,\n",
       "  25,\n",
       "  344,\n",
       "  103,\n",
       "  27,\n",
       "  603,\n",
       "  275,\n",
       "  61,\n",
       "  19,\n",
       "  253,\n",
       "  669,\n",
       "  27,\n",
       "  583,\n",
       "  90,\n",
       "  1016,\n",
       "  154,\n",
       "  237,\n",
       "  6,\n",
       "  1001,\n",
       "  1004,\n",
       "  8,\n",
       "  92,\n",
       "  335,\n",
       "  69,\n",
       "  303,\n",
       "  371,\n",
       "  28,\n",
       "  189,\n",
       "  117,\n",
       "  996,\n",
       "  17,\n",
       "  1005,\n",
       "  168,\n",
       "  267,\n",
       "  96,\n",
       "  131,\n",
       "  45,\n",
       "  517,\n",
       "  788,\n",
       "  25,\n",
       "  112,\n",
       "  129,\n",
       "  10,\n",
       "  44,\n",
       "  118,\n",
       "  1000,\n",
       "  469,\n",
       "  71,\n",
       "  422,\n",
       "  27,\n",
       "  256,\n",
       "  491,\n",
       "  25,\n",
       "  159,\n",
       "  324,\n",
       "  115,\n",
       "  129,\n",
       "  190,\n",
       "  69,\n",
       "  383,\n",
       "  27,\n",
       "  256,\n",
       "  306,\n",
       "  171,\n",
       "  999],\n",
       " [202,\n",
       "  71,\n",
       "  209,\n",
       "  226,\n",
       "  32,\n",
       "  156,\n",
       "  28,\n",
       "  256,\n",
       "  444,\n",
       "  25,\n",
       "  397,\n",
       "  87,\n",
       "  114,\n",
       "  256,\n",
       "  231,\n",
       "  150,\n",
       "  46,\n",
       "  72,\n",
       "  167,\n",
       "  998,\n",
       "  101,\n",
       "  8,\n",
       "  142,\n",
       "  443,\n",
       "  143,\n",
       "  76,\n",
       "  28,\n",
       "  8,\n",
       "  837,\n",
       "  1017,\n",
       "  169,\n",
       "  1011],\n",
       " [141,\n",
       "  71,\n",
       "  568,\n",
       "  27,\n",
       "  494,\n",
       "  9,\n",
       "  318,\n",
       "  849,\n",
       "  29,\n",
       "  1000,\n",
       "  1017,\n",
       "  435,\n",
       "  1004,\n",
       "  17,\n",
       "  278,\n",
       "  87,\n",
       "  158,\n",
       "  662,\n",
       "  42,\n",
       "  282,\n",
       "  28,\n",
       "  784,\n",
       "  250,\n",
       "  456,\n",
       "  32,\n",
       "  27,\n",
       "  7,\n",
       "  1017,\n",
       "  1000,\n",
       "  64,\n",
       "  900,\n",
       "  101,\n",
       "  114,\n",
       "  35,\n",
       "  697,\n",
       "  114,\n",
       "  463,\n",
       "  25,\n",
       "  463,\n",
       "  58,\n",
       "  715,\n",
       "  90,\n",
       "  1002,\n",
       "  400,\n",
       "  114,\n",
       "  61,\n",
       "  35,\n",
       "  298,\n",
       "  114,\n",
       "  112,\n",
       "  33,\n",
       "  36,\n",
       "  217,\n",
       "  62,\n",
       "  638,\n",
       "  94]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bhyp_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but i think i will remember not to do it again\n",
      "but i think i will remember not to do it again\n",
      "\n",
      "he will see no change it is true my young men did not go out on the war path they had dreams for not doing so but they love and venerate the great white chief\n",
      "he will see no change it is true my young men did not go out on the war path they had dreams for not doing so but they love and venerate the great white chief\n",
      "\n",
      "please to return to my service well and good but to suppose that i am going to disturb or unhens the ancient usage of knight errantry is all nonsense and so my sancho get you back to your house and explain my intentions to your teresa\n",
      "please to return to my service well and good but to suppose that i am going to disturb or unhinge the ancient usage of knight errantry is all nonsense and so my sancho get you back to your house and explain my intentions to your teresa\n",
      "\n",
      "if you will bring one of your men and come with me yourself said gimblet at the conclusion of the interview\n",
      "if you will bring one of your men and come with me yourself said gimblet at the conclusion of the interview\n",
      "\n",
      "do you want to join too simonov observed with no appearance of pleasure seeming to avoid looking at me he knew me through and through it infuriated me that he know me so thoroughly why not\n",
      "do you want to join too simonov observed with no appearance of pleasure seeming to avoid looking at me he knew me through and through it infuriated me that he knew me so thoroughly why not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(benc_input.shape[0]):\n",
    "    print(bhyp_text[i])\n",
    "    print(bans_text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9954, 0.7857,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 0.9913, 0.9820,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 0.9791, 0.9395,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.9745, 0.9999, 0.9968,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9396, 0.9999, 0.9977,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9995, 0.9949, 0.9989,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:4')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# calc softmax\n",
    "p = torch.exp(bpadded_output) / torch.exp(bpadded_output).sum(dim=-1, keepdim=True)\n",
    "p[0][:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logp[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logp' is not defined"
     ]
    }
   ],
   "source": [
    "logp[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m enc_output, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m      2\u001b[0m                 benc_input[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor([benc_input[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)])\n\u001b[1;32m      3\u001b[0m             )  \u001b[39m# [1, subsampled_enc_input_length, output_size]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pred_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[tokenizer\u001b[39m.\u001b[39mblank_token_id]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mto(enc_output\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m pred_output, hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mforward_wo_prepend(pred_input, torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m]), hidden\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "enc_output, _ = model.encoder(\n",
    "                benc_input[0].unsqueeze(0), torch.tensor([benc_input[0].size(0)])\n",
    "            )  # [1, subsampled_enc_input_length, output_size]\n",
    "pred_input = torch.tensor([[tokenizer.blank_token_id]], dtype=torch.int32).to(enc_output.device)\n",
    "pred_output, hidden = model.predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=None)\n",
    "timestamp = 0\n",
    "hyp_tokens = []\n",
    "while timestamp < enc_output.shape[1]:\n",
    "    enc_output_at_t = enc_output[0, timestamp, :]\n",
    "    logits = model.jointnet(enc_output_at_t.view(1, 1, -1), pred_output)\n",
    "    pred_token = logits.argmax(dim=-1)\n",
    "    if timestamp == 0:\n",
    "        break\n",
    "    if pred_token != tokenizer.blank_token_id:\n",
    "        hyp_tokens.append(pred_token.item())\n",
    "        pred_input = torch.tensor([[pred_token]], dtype=torch.int32).to(enc_output.device)\n",
    "        pred_output, hidden = model.predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=hidden)\n",
    "    else:\n",
    "        timestamp += 1\n",
    "    \n",
    "    if len(hyp_tokens) > 100:\n",
    "        print(\"detect\")\n",
    "        break\n",
    "print(torch.sort(logits[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁at'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "hyp_token_indices = []\n",
    "for i in tqdm(range(7, enc_input.shape[0])):\n",
    "    enc_x = enc_input[:i, :].unsqueeze(0)\n",
    "    enc_x_len = torch.tensor([i])\n",
    "    enc_y, enc_sub_x_len = model.encoder(enc_x, enc_x_len)\n",
    "    joint_x = enc_y[0, -1, :].unsqueeze(0).unsqueeze(0)\n",
    "    num_token_indices = 0\n",
    "    while True:\n",
    "        logits = model.jointnet(joint_x, pred_output)\n",
    "        hyp = torch.argmax(logits, dim=-1)\n",
    "        if hyp == tokenizer.blank_token_id:\n",
    "            break\n",
    "        else:\n",
    "            hyp_token_indices.append(hyp.item())\n",
    "            pred_input = torch.tensor([[hyp]], dtype=torch.int32).to(enc_input.device)\n",
    "            pred_output, hidden = model.predictor.forward_wo_prepend(\n",
    "                pred_input, torch.tensor([1]), hidden=hidden\n",
    "            )\n",
    "            num_token_indices += 1\n",
    "        \n",
    "        if num_token_indices > 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "hyp_token_indices = []\n",
    "for i in tqdm(range(8, enc_input.shape[0])):\n",
    "    enc_x = enc_input[:i, :].unsqueeze(0)\n",
    "    enc_x_len = torch.tensor([i])\n",
    "    enc_y, enc_sub_x_len = model.encoder(enc_x, enc_x_len)\n",
    "    break\n",
    "    batch_enc_output, batch_subsampled_length = model.encoder(batch, batch_lengths)\n",
    "    subsampled_length = batch_subsampled_length[0]\n",
    "    # NOTE: JointNetは線形層を通しているだけであり、時刻に関して独立->現在のenc_outだけで十分\n",
    "    enc_output = batch_enc_output[0][subsampled_length - 1].view(1, 1, -1)\n",
    "    num_token_indices = 0\n",
    "    while True:\n",
    "        logits = model.jointnet(enc_output, pred_output)[0]\n",
    "        pred_token_idx = torch.argmax(logits, dim=-1)\n",
    "        if pred_token_idx == model.blank_idx:\n",
    "            break\n",
    "        else:\n",
    "            num_token_indices += 1\n",
    "            hyp_token_indices.append(pred_token_idx.item())\n",
    "            pred_input = torch.tensor([[pred_token_idx]], dtype=torch.int32).to(enc_input.device)\n",
    "            pred_output, hidden = model.predictor.forward_wo_prepend(\n",
    "                pred_input, torch.tensor([1]), hidden=hidden\n",
    "            )\n",
    "\n",
    "        if num_token_indices >= 5:\n",
    "            break\n",
    "hyp_text = tokenizer.token_ids_to_text(hyp_token_indices)\n",
    "print(hyp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    hyp = model.streaming_greedy_inference(\n",
    "        enc_inputs=benc_input[0].unsqueeze(0), enc_input_lengths=[benc_input_length[0]]\n",
    "    )\n",
    "    hyp_text = tokenizer.batch_token_ids_to_text(hyp)\n",
    "    print(hyp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bans_token_indices = [\n",
    "    bpred_input[i, : bpred_input_length[i]].tolist() for i in range(bpred_input.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_token_ids_to_text(bans_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_token_ids_to_text([hyp_token_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = model(\n",
    "    padded_enc_input=benc_input,\n",
    "    enc_input_lengths=benc_input_length,\n",
    "    padded_pred_input=bpred_input,\n",
    "    pred_input_lengths=bpred_input_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0].argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_idx = []\n",
    "for bidx, bx, by, bx_len, by_len, baudio_sec in dataloader:\n",
    "    sampled_idx.append(bidx)\n",
    "    print(sum(baudio_sec))\n",
    "print(sampled_idx)\n",
    "sampled_indices = [item for sublist in sampled_idx for item in sublist]\n",
    "print(len(set(sampled_indices)) == len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_dataset = LibriSpeechDataset(\n",
    "    json_file_path=\"./json/librispeech_dev-other.json\", resampling_rate=16000, tokenizer=tokenizer\n",
    ")\n",
    "libri_dataloader = get_dataloader(\n",
    "    libri_dataset,\n",
    "    batch_sec=100,\n",
    "    num_workers=1,\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_idx = []\n",
    "for bidx, bx, by, bx_len, by_len, baudio_sec in libri_dataloader:\n",
    "    sampled_idx.append(bidx)\n",
    "    print(sum(baudio_sec))\n",
    "print(sampled_idx)\n",
    "sampled_indices = [item for sublist in sampled_idx for item in sublist]\n",
    "print(len(set(sampled_indices)) == len(libri_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 10, 5)\n",
    "input_length=x.shape[1]\n",
    "future_mask = torch.triu(torch.ones(input_length, input_length), diagonal=1).bool()\n",
    "print(future_mask)\n",
    "NUM_PREVIOUS_FRAMES = \"all\"\n",
    "# mask before NUM_PREVIOUS_FRAMES\n",
    "input_length=x.shape[1]\n",
    "if NUM_PREVIOUS_FRAMES == \"all\":\n",
    "    previous_mask = torch.zeros(input_length, input_length).bool()\n",
    "else:\n",
    "    previous_mask=torch.tril(torch.ones(input_length, input_length), diagonal=-(NUM_PREVIOUS_FRAMES+1)).bool()\n",
    "future_and_previous_mask = torch.logical_or(future_mask, previous_mask)\n",
    "print(future_and_previous_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, 10, 5) # [B, D, T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum = x.cumsum(dim=-1).sum(dim=0).repeat(x.shape[0], 1, 1)\n",
    "cum_num_element = (\n",
    "    (torch.arange(1, x.shape[-1] + 1) * x.shape[0]).repeat(x.shape[0], x.shape[1], 1).to(x.device)\n",
    ")\n",
    "cum_mean = cum_sum / cum_num_element\n",
    "cum_var = ((x - cum_mean) ** 2).cumsum(dim=-1).sum(dim=0).repeat(x.shape[0], 1, 1) / cum_num_element\n",
    "cum_std = torch.sqrt(cum_var + self.eps)\n",
    "cum_std = cum_std + self.eps\n",
    "normalized_x = (x - cum_mean) / cum_std\n",
    "if self.affine:\n",
    "    normalized_x = normalized_x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "time_and_chan_wise_sum = x.sum(dim=0).unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "time_and_chan_wise_mean = time_and_chan_wise_sum / x.shape[0]\n",
    "time_and_chan_wise_var = ((x - time_and_chan_wise_mean) ** 2).sum(dim=0).unsqueeze(0).repeat(x.shape[0], 1, 1) / x.shape[0]\n",
    "time_and_chan_wise_std = torch.sqrt(time_and_chan_wise_var + eps)\n",
    "time_and_chan_wise_std = time_and_chan_wise_std + eps\n",
    "normalized_x = (x - time_and_chan_wise_mean) / time_and_chan_wise_std\n",
    "#if self.affine:\n",
    "#    normalized_x = normalized_x * self.gamma + self.beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "time_and_batch_wise_sum = x.sum(dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "time_and_batch_wise_mean = time_and_batch_wise_sum / x.shape[1]\n",
    "time_and_batch_wise_var = ((x - time_and_batch_wise_mean) ** 2).sum(dim=1).unsqueeze(1).repeat(1, x.shape[1], 1) / x.shape[1]\n",
    "time_and_batch_wise_std = torch.sqrt(time_and_batch_wise_var + eps)\n",
    "time_and_batch_wise_std = time_and_batch_wise_std + eps\n",
    "normalized_x = (x - time_and_batch_wise_mean) / time_and_batch_wise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 10, (3, 10, 5))\n",
    "x # [B, T, D]\n",
    "x = x.transpose(1, 2) # [B, D, T]\n",
    "gamma = torch.ones(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = x.transpose(1, 2) # [B, T, D]\n",
    "n_x = n_x * gamma\n",
    "n_x = n_x.transpose(1, 2) # [B, D, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
