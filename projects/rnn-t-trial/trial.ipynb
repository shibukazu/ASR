{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "import torch.nn\n",
    "from data import YesNoDataset\n",
    "from model import Model\n",
    "from torchaudio.functional import rnnt_loss\n",
    "from torch.nn.functional import log_softmax\n",
    "from torchmetrics.functional import char_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YesNoDataset(\n",
    "    wav_dir_path=\"datasets/waves_yesno/\",\n",
    "    model_sample_rate=16000,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label = {v: k for k, v in dataset.label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': 0, 'y': 1, 'e': 2, 's': 3, 'n': 4, 'o': 5, '<space>': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.label_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Model(\n",
    "    vocab_size=7,\n",
    "    encoder_input_size=40,\n",
    "    encoder_hidden_size=64,\n",
    "    encoder_num_layers=1,\n",
    "    embedding_size=64,\n",
    "    predictor_hidden_size=64,\n",
    "    predictor_num_layers=1,\n",
    "    jointnet_hidden_size=64,\n",
    "    blank_idx=0,\n",
    ").to(DEVICE)\n",
    "checkpoint = torch.load(\"model_YesNo.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.encoder\n",
    "predictor = model.predictor\n",
    "jointnet = model.jointnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_specs, tokens, mel_spec_lengths, token_lengths = next(iter(dataloader))\n",
    "mel_specs = mel_specs.to(DEVICE)\n",
    "tokens = tokens.to(DEVICE)\n",
    "\n",
    "enc_inputs = mel_specs\n",
    "enc_input_lengths = mel_spec_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp_tokens: [1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3]\n",
      "tokens    : [1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3]\n",
      "hyp_tokens: [1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5]\n",
      "tokens    : [1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5]\n",
      "hyp_tokens: [1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3]\n",
      "tokens    : [1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3]\n",
      "hyp_tokens: [1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5]\n",
      "tokens    : [1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5]\n",
      "hyp_tokens: [4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 4, 5]\n",
      "tokens    : [4, 5, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5, 6, 4, 5]\n",
      "hyp_tokens: [4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3]\n",
      "tokens    : [4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3]\n",
      "hyp_tokens: [4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5]\n",
      "tokens    : [4, 5, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 4, 5]\n",
      "hyp_tokens: [1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5]\n",
      "tokens    : [1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 6, 1, 2, 3, 6, 4, 5, 6, 1, 2, 3, 6, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_hyp_tokens = []\n",
    "    for i, (enc_input, enc_input_length) in enumerate(zip(enc_inputs, enc_input_lengths)):\n",
    "        if enc_input.size(0) > enc_input_length:\n",
    "            enc_input = enc_input[:enc_input_length, :]\n",
    "\n",
    "        enc_output, _ = encoder(\n",
    "            enc_input.unsqueeze(0), torch.tensor([enc_input.size(0)])\n",
    "        )  # [1, subsampled_enc_input_length, output_size]\n",
    "        pred_input = torch.tensor([[0]], dtype=torch.int32).to(enc_output.device)\n",
    "        pred_output, hidden = predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=None)\n",
    "        # [1, 1, output_size]\n",
    "        timestamp = 0\n",
    "        hyp_tokens = []\n",
    "        while timestamp < enc_output.size(1):\n",
    "            enc_output_at_timestamp = enc_output[0, timestamp]\n",
    "            logits = jointnet(enc_output_at_timestamp.view(1, 1, -1), pred_output)\n",
    "            pred_token = logits.argmax(dim=-1)\n",
    "            if pred_token != 0:\n",
    "                hyp_tokens.append(pred_token.item())\n",
    "                pred_input = torch.tensor([[pred_token]], dtype=torch.int32).to(enc_output.device)\n",
    "                pred_output, hidden = predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=hidden)\n",
    "            else:\n",
    "                timestamp += 1\n",
    "        batch_hyp_tokens.append(hyp_tokens)\n",
    "        print(\"hyp_tokens:\", hyp_tokens)\n",
    "        print(\"tokens    :\", tokens[i, :token_lengths[i]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hyp_tokens\n",
    "hyp_texts = [\"\".join([idx_to_label[idx] for idx in hyp_token]) for hyp_token in batch_hyp_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_tokens = [tokens[i, : token_lengths[i]].tolist() for i in range(tokens.shape[0])]\n",
    "ans_texts = [\"\".join([idx_to_label[idx] for idx in ans_token]) for ans_token in ans_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1993)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_error_rate(hyp_texts, ans_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = dataset.label_to_idx\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Predictor' object has no attribute 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m pred_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 4\u001b[0m pred_output, hidden \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49minference(pred_input)\n\u001b[1;32m      5\u001b[0m timestamp \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      6\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1176\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Predictor' object has no attribute 'inference'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# pred_input: [1, B, 1]\n",
    "model.eval()\n",
    "pred_input = torch.tensor([0], dtype=torch.int32).to(DEVICE)\n",
    "pred_output, hidden = predictor.inference(pred_input)\n",
    "timestamp = 0\n",
    "results = []\n",
    "while timestamp < encoder_output.shape[0]:\n",
    "    encoder_output_t = encoder_output[timestamp].view(1,1,-1)\n",
    "    # 各encoder_output[i]は内部的に過去の情報を持っているため、jointで渡すのは現時刻の潜在表現で良い\n",
    "    joint_output = jointnet(encoder_output_t, pred_output)\n",
    "    pred_token_idx = torch.argmax(joint_output, dim=-1)\n",
    "    if pred_token_idx == 0:\n",
    "        timestamp += 1\n",
    "    else:\n",
    "        results.append(pred_token_idx.view(1).item())\n",
    "        pred_input = torch.tensor([pred_token_idx], dtype=torch.int32).to(DEVICE)\n",
    "        pred_output, hidden = predictor.inference(pred_input, hidden)\n",
    "\n",
    "print(results)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
