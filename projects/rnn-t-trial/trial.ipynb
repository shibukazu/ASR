{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "import torch.nn\n",
    "from data import YesNoDataset, LibriSpeechDataset, get_dataloader\n",
    "from model import CausalConformerModel, TorchAudioConformerModel\n",
    "from torchaudio.functional import rnnt_loss\n",
    "from torch.nn.functional import log_softmax\n",
    "from torchmetrics.functional import char_error_rate\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tokenizer import SentencePieceTokenizer\n",
    "from create_json import create_librispeech_json\n",
    "from modules import torchaudio_conformer\n",
    "from modules.conformer.normalization import TimewiseBatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSentencePieceTokenizer.create_model(\\n    transcription_file_path=\"vocabs/librispeech_train_960h_transcripts.txt\",\\n    model_prefix=\"librispeech_char\",\\n    num_tokens=1024,\\n    model_type=\"char\",\\n    character_coverage=1.0,\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SentencePieceTokenizer.create_model(\n",
    "    transcription_file_path=\"vocabs/librispeech_train_960h_transcripts.txt\",\n",
    "    model_prefix=\"librispeech_char\",\n",
    "    num_tokens=1024,\n",
    "    model_type=\"char\",\n",
    "    character_coverage=1.0,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm = TimewiseBatchNormalization(input_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5375],\n",
      "         [ 2.1648],\n",
      "         [-1.5052]]])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_norm.train()\n",
    "x = torch.randn(1, 3, 1)\n",
    "print(x)\n",
    "print(batch_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.],\n",
       "         [0.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_and_chan_wise_mean = batch_norm.moving_mean\n",
    "time_and_chan_wise_var = batch_norm.moving_var\n",
    "time_and_chan_wise_std = torch.sqrt(time_and_chan_wise_var + batch_norm.eps)\n",
    "time_and_chan_wise_std = time_and_chan_wise_std + batch_norm.eps\n",
    "normalized_x = (x - time_and_chan_wise_mean) / time_and_chan_wise_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1630],\n",
      "         [-1.8460],\n",
      "         [ 0.0052]]])\n",
      "tensor([[[ -3744.7776],\n",
      "         [-40104.4258],\n",
      "         [ 15103.0430]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_norm.eval()\n",
    "x = torch.randn(1, 3, 1)\n",
    "print(x)\n",
    "print(batch_norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7907, -1.0085,  1.5201,  0.0829],\n",
       "         [ 0.7827, -1.2146,  0.3603,  0.6646],\n",
       "         [-2.3024, -1.9154, -0.2944,  0.4802]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_norm.moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./artifacts/librispeech_small_test/a3ed4353137f48a39a789451847375ac/artifacts/model_70.pth\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    cpt = torch.load(f)\n",
    "model_state = cpt[\"model\"]\n",
    "model_args = cpt[\"model_args\"]\n",
    "model = TorchAudioConformerModel(**model_args).to(DEVICE)\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./librispeech_small/artifacts/f4291c590aba4faf8aff7947af9af524/artifacts/model_20.pth\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    cpt = torch.load(f)\n",
    "model_state = cpt[\"model\"]\n",
    "model_args = cpt[\"model_args\"]\n",
    "model = CausalConformerModel(**model_args).to(DEVICE)\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Prepare: 100%|██████████| 28539/28539 [00:00<00:00, 352943.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_file_path=\"./vocabs/librispeech_1024_bpe.model\"\n",
    ")\n",
    "dataset = LibriSpeechDataset(\n",
    "    resampling_rate=16000,\n",
    "    tokenizer=tokenizer,\n",
    "    json_file_path=\"./json/librispeech_train-clean-100.json\",\n",
    ")\n",
    "dataloader = get_dataloader(\n",
    "    dataset,\n",
    "    batch_sec=60,\n",
    "    num_workers=1,\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "iterator = iter(dataloader)\n",
    "for _ in range(30):\n",
    "    next(iterator)\n",
    "_, benc_input, bpred_input, benc_input_length, bpred_input_length, baudio_sec = next(iterator)\n",
    "benc_input = benc_input.to(DEVICE)\n",
    "bpred_input = bpred_input.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.7601, device='cuda:4')\n",
      "hyp: the explained also what they wanted by acting as if they had a piece of blubber in their mouths and then pretending to cut instead of territ i have not as yet noticed the four years whom we had on board\n",
      "ans: they explained also what they wanted by acting as if they had a piece of blubber in their mouth and then pretending to cut instead of tear it i have not as yet noticed the fuegians whom we had on board\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    bpadded_output, bpadded_ctc_log_probs, bsubsampled_enc_input_length = model(\n",
    "        padded_enc_input=benc_input,\n",
    "        enc_input_lengths=benc_input_length,\n",
    "        padded_pred_input=bpred_input,\n",
    "        pred_input_lengths=bpred_input_length,\n",
    "    )\n",
    "    loss = rnnt_loss(\n",
    "        logits=bpadded_output,\n",
    "        targets=bpred_input,\n",
    "        logit_lengths=bsubsampled_enc_input_length.to(DEVICE),\n",
    "        target_lengths=bpred_input_length.to(DEVICE),\n",
    "        blank=tokenizer.blank_token_id,\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "    print(loss / bpred_input.shape[0])\n",
    "    bhyp_token_indices = model.greedy_inference(\n",
    "        enc_inputs=benc_input[0].unsqueeze(0), enc_input_lengths=[benc_input_length[0]]\n",
    "    )\n",
    "    bhyp_text = tokenizer.batch_token_ids_to_text(bhyp_token_indices)\n",
    "    bans_token_indices = [\n",
    "        bpred_input[i, : bpred_input_length[i]].tolist() for i in range(bpred_input.shape[0])\n",
    "    ]\n",
    "    bhyp_text = tokenizer.batch_token_ids_to_text(bhyp_token_indices)\n",
    "    bans_text = tokenizer.batch_token_ids_to_text(bans_token_indices)\n",
    "    for hyp_text, ans_text in zip(bhyp_text, bans_text):\n",
    "        print(f\"hyp: {hyp_text}\")\n",
    "        print(f\"ans: {ans_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9954, 0.7857,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 0.9913, 0.9820,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 0.9791, 0.9395,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [0.9745, 0.9999, 0.9968,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9396, 0.9999, 0.9977,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.9995, 0.9949, 0.9989,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       device='cuda:4')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# calc softmax\n",
    "p = torch.exp(bpadded_output) / torch.exp(bpadded_output).sum(dim=-1, keepdim=True)\n",
    "p[0][:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0266e-06, -2.5104e+01, -2.5104e+01,  ..., -3.5290e+01,\n",
       "        -2.8254e+01, -3.2530e+01], device='cuda:4')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([-34.1344, -32.8127, -32.6079,  ...,  -5.3108,  -3.6692,   9.8395],\n",
      "       device='cuda:4', grad_fn=<SortBackward0>),\n",
      "indices=tensor([631, 278, 879,  ..., 101,  25,   0], device='cuda:4'))\n"
     ]
    }
   ],
   "source": [
    "enc_output, _ = model.encoder(\n",
    "                benc_input[0].unsqueeze(0), torch.tensor([benc_input[0].size(0)])\n",
    "            )  # [1, subsampled_enc_input_length, output_size]\n",
    "pred_input = torch.tensor([[tokenizer.blank_token_id]], dtype=torch.int32).to(enc_output.device)\n",
    "pred_output, hidden = model.predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=None)\n",
    "timestamp = 0\n",
    "hyp_tokens = []\n",
    "while timestamp < enc_output.shape[1]:\n",
    "    enc_output_at_t = enc_output[0, timestamp, :]\n",
    "    logits = model.jointnet(enc_output_at_t.view(1, 1, -1), pred_output)\n",
    "    pred_token = logits.argmax(dim=-1)\n",
    "    if timestamp == 0:\n",
    "        break\n",
    "    if pred_token != tokenizer.blank_token_id:\n",
    "        hyp_tokens.append(pred_token.item())\n",
    "        pred_input = torch.tensor([[pred_token]], dtype=torch.int32).to(enc_output.device)\n",
    "        pred_output, hidden = model.predictor.forward_wo_prepend(pred_input, torch.tensor([1]), hidden=hidden)\n",
    "    else:\n",
    "        timestamp += 1\n",
    "    \n",
    "    if len(hyp_tokens) > 100:\n",
    "        print(\"detect\")\n",
    "        break\n",
    "print(torch.sort(logits[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁at'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "hyp_token_indices = []\n",
    "for i in tqdm(range(7, enc_input.shape[0])):\n",
    "    enc_x = enc_input[:i, :].unsqueeze(0)\n",
    "    enc_x_len = torch.tensor([i])\n",
    "    enc_y, enc_sub_x_len = model.encoder(enc_x, enc_x_len)\n",
    "    joint_x = enc_y[0, -1, :].unsqueeze(0).unsqueeze(0)\n",
    "    num_token_indices = 0\n",
    "    while True:\n",
    "        logits = model.jointnet(joint_x, pred_output)\n",
    "        hyp = torch.argmax(logits, dim=-1)\n",
    "        if hyp == tokenizer.blank_token_id:\n",
    "            break\n",
    "        else:\n",
    "            hyp_token_indices.append(hyp.item())\n",
    "            pred_input = torch.tensor([[hyp]], dtype=torch.int32).to(enc_input.device)\n",
    "            pred_output, hidden = model.predictor.forward_wo_prepend(\n",
    "                pred_input, torch.tensor([1]), hidden=hidden\n",
    "            )\n",
    "            num_token_indices += 1\n",
    "        \n",
    "        if num_token_indices > 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_token_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "hyp_token_indices = []\n",
    "for i in tqdm(range(8, enc_input.shape[0])):\n",
    "    enc_x = enc_input[:i, :].unsqueeze(0)\n",
    "    enc_x_len = torch.tensor([i])\n",
    "    enc_y, enc_sub_x_len = model.encoder(enc_x, enc_x_len)\n",
    "    break\n",
    "    batch_enc_output, batch_subsampled_length = model.encoder(batch, batch_lengths)\n",
    "    subsampled_length = batch_subsampled_length[0]\n",
    "    # NOTE: JointNetは線形層を通しているだけであり、時刻に関して独立->現在のenc_outだけで十分\n",
    "    enc_output = batch_enc_output[0][subsampled_length - 1].view(1, 1, -1)\n",
    "    num_token_indices = 0\n",
    "    while True:\n",
    "        logits = model.jointnet(enc_output, pred_output)[0]\n",
    "        pred_token_idx = torch.argmax(logits, dim=-1)\n",
    "        if pred_token_idx == model.blank_idx:\n",
    "            break\n",
    "        else:\n",
    "            num_token_indices += 1\n",
    "            hyp_token_indices.append(pred_token_idx.item())\n",
    "            pred_input = torch.tensor([[pred_token_idx]], dtype=torch.int32).to(enc_input.device)\n",
    "            pred_output, hidden = model.predictor.forward_wo_prepend(\n",
    "                pred_input, torch.tensor([1]), hidden=hidden\n",
    "            )\n",
    "\n",
    "        if num_token_indices >= 5:\n",
    "            break\n",
    "hyp_text = tokenizer.token_ids_to_text(hyp_token_indices)\n",
    "print(hyp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    hyp = model.streaming_greedy_inference(\n",
    "        enc_inputs=benc_input[0].unsqueeze(0), enc_input_lengths=[benc_input_length[0]]\n",
    "    )\n",
    "    hyp_text = tokenizer.batch_token_ids_to_text(hyp)\n",
    "    print(hyp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bans_token_indices = [\n",
    "    bpred_input[i, : bpred_input_length[i]].tolist() for i in range(bpred_input.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_token_ids_to_text(bans_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_token_ids_to_text([hyp_token_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = model(\n",
    "    padded_enc_input=benc_input,\n",
    "    enc_input_lengths=benc_input_length,\n",
    "    padded_pred_input=bpred_input,\n",
    "    pred_input_lengths=bpred_input_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.blank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0].argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_idx = []\n",
    "for bidx, bx, by, bx_len, by_len, baudio_sec in dataloader:\n",
    "    sampled_idx.append(bidx)\n",
    "    print(sum(baudio_sec))\n",
    "print(sampled_idx)\n",
    "sampled_indices = [item for sublist in sampled_idx for item in sublist]\n",
    "print(len(set(sampled_indices)) == len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_dataset = LibriSpeechDataset(\n",
    "    json_file_path=\"./json/librispeech_dev-other.json\", resampling_rate=16000, tokenizer=tokenizer\n",
    ")\n",
    "libri_dataloader = get_dataloader(\n",
    "    libri_dataset,\n",
    "    batch_sec=100,\n",
    "    num_workers=1,\n",
    "    pad_idx=tokenizer.pad_token_id,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_idx = []\n",
    "for bidx, bx, by, bx_len, by_len, baudio_sec in libri_dataloader:\n",
    "    sampled_idx.append(bidx)\n",
    "    print(sum(baudio_sec))\n",
    "print(sampled_idx)\n",
    "sampled_indices = [item for sublist in sampled_idx for item in sublist]\n",
    "print(len(set(sampled_indices)) == len(libri_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 10, 5)\n",
    "input_length=x.shape[1]\n",
    "future_mask = torch.triu(torch.ones(input_length, input_length), diagonal=1).bool()\n",
    "print(future_mask)\n",
    "NUM_PREVIOUS_FRAMES = \"all\"\n",
    "# mask before NUM_PREVIOUS_FRAMES\n",
    "input_length=x.shape[1]\n",
    "if NUM_PREVIOUS_FRAMES == \"all\":\n",
    "    previous_mask = torch.zeros(input_length, input_length).bool()\n",
    "else:\n",
    "    previous_mask=torch.tril(torch.ones(input_length, input_length), diagonal=-(NUM_PREVIOUS_FRAMES+1)).bool()\n",
    "future_and_previous_mask = torch.logical_or(future_mask, previous_mask)\n",
    "print(future_and_previous_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, 10, 5) # [B, D, T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum = x.cumsum(dim=-1).sum(dim=0).repeat(x.shape[0], 1, 1)\n",
    "cum_num_element = (\n",
    "    (torch.arange(1, x.shape[-1] + 1) * x.shape[0]).repeat(x.shape[0], x.shape[1], 1).to(x.device)\n",
    ")\n",
    "cum_mean = cum_sum / cum_num_element\n",
    "cum_var = ((x - cum_mean) ** 2).cumsum(dim=-1).sum(dim=0).repeat(x.shape[0], 1, 1) / cum_num_element\n",
    "cum_std = torch.sqrt(cum_var + self.eps)\n",
    "cum_std = cum_std + self.eps\n",
    "normalized_x = (x - cum_mean) / cum_std\n",
    "if self.affine:\n",
    "    normalized_x = normalized_x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "time_and_chan_wise_sum = x.sum(dim=0).unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "time_and_chan_wise_mean = time_and_chan_wise_sum / x.shape[0]\n",
    "time_and_chan_wise_var = ((x - time_and_chan_wise_mean) ** 2).sum(dim=0).unsqueeze(0).repeat(x.shape[0], 1, 1) / x.shape[0]\n",
    "time_and_chan_wise_std = torch.sqrt(time_and_chan_wise_var + eps)\n",
    "time_and_chan_wise_std = time_and_chan_wise_std + eps\n",
    "normalized_x = (x - time_and_chan_wise_mean) / time_and_chan_wise_std\n",
    "#if self.affine:\n",
    "#    normalized_x = normalized_x * self.gamma + self.beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "time_and_batch_wise_sum = x.sum(dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "time_and_batch_wise_mean = time_and_batch_wise_sum / x.shape[1]\n",
    "time_and_batch_wise_var = ((x - time_and_batch_wise_mean) ** 2).sum(dim=1).unsqueeze(1).repeat(1, x.shape[1], 1) / x.shape[1]\n",
    "time_and_batch_wise_std = torch.sqrt(time_and_batch_wise_var + eps)\n",
    "time_and_batch_wise_std = time_and_batch_wise_std + eps\n",
    "normalized_x = (x - time_and_batch_wise_mean) / time_and_batch_wise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 10, (3, 10, 5))\n",
    "x # [B, T, D]\n",
    "x = x.transpose(1, 2) # [B, D, T]\n",
    "gamma = torch.ones(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = x.transpose(1, 2) # [B, T, D]\n",
    "n_x = n_x * gamma\n",
    "n_x = n_x.transpose(1, 2) # [B, D, T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
