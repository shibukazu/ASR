{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "import torch.nn\n",
    "from data import YesNoDataset\n",
    "from torchaudio.functional import rnnt_loss\n",
    "from torch.nn.functional import log_softmax\n",
    "from torchmetrics.functional import char_error_rate\n",
    "from modules.conformer.convolution import CausalConvolutionLayer\n",
    "from modules.subsampling import Conv2DSubSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "dataset = YesNoDataset(\n",
    "    wav_dir_path=\"datasets/waves_yesno/\",\n",
    "    model_sample_rate=16000,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    drop_last=False,\n",
    ")\n",
    "num_labels = len(dataset.idx_to_token)\n",
    "token_to_idx = dataset.token_to_idx\n",
    "blank_idx = dataset.blank_idx\n",
    "pad_idx = dataset.pad_idx\n",
    "idx_to_token = dataset.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_length, subsampled_input_length, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.subsampled_input_length = subsampled_input_length\n",
    "        self.num_labels = num_labels\n",
    "        self.subsampling = Conv2DSubSampling(input_length, subsampled_input_length, 3, 2, 3, 2)\n",
    "        self.causal_conv = CausalConvolutionLayer(\n",
    "            input_channels=subsampled_input_length,\n",
    "            hidden_channels=32,\n",
    "            depthwise_kernel_size=12,\n",
    "            dropout=0\n",
    "        ) # [B, T, D]\n",
    "        self.fc = torch.nn.Linear(subsampled_input_length, num_labels)\n",
    "    \n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        subsampled_padded_input, subsampled_input_lengths = self.subsampling(padded_input, input_lengths)\n",
    "        padded_output = self.causal_conv(subsampled_padded_input)\n",
    "        padded_output = self.fc(padded_output)\n",
    "        padded_log_prob = log_softmax(padded_output, dim=2)\n",
    "        return padded_log_prob, subsampled_input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(hypotheses_idxs, idx_to_token, padding_idx, blank_idx):\n",
    "\n",
    "    hypotheses_idxs = hypotheses_idxs.cpu().numpy()\n",
    "    hypotheses = []\n",
    "    for hypothesis_idxs in hypotheses_idxs:\n",
    "        hypothesis = []\n",
    "        prev_idx = -1\n",
    "        for idx in hypothesis_idxs:\n",
    "            if idx == blank_idx:\n",
    "                continue\n",
    "            elif idx == prev_idx:\n",
    "                continue\n",
    "            elif idx == padding_idx:\n",
    "                continue\n",
    "            else:\n",
    "                hypothesis.append(idx_to_token[idx])\n",
    "                prev_idx = idx\n",
    "        hypotheses.append(\"\".join(hypothesis))\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: 175.83381366729736 loss, CER: 0.9370034337043762, 1.3335866928100586 sec\n",
      "1 epoch: 141.56728553771973 loss, CER: 0.7443107962608337, 0.48404765129089355 sec\n",
      "2 epoch: 105.41268348693848 loss, CER: 0.9437316656112671, 0.465836763381958 sec\n",
      "3 epoch: 70.4853253364563 loss, CER: 0.9689525961875916, 0.48598313331604004 sec\n",
      "4 epoch: 53.91745448112488 loss, CER: 0.9706601500511169, 0.39849090576171875 sec\n",
      "5 epoch: 51.39614009857178 loss, CER: 0.9695340394973755, 1.102468729019165 sec\n",
      "6 epoch: 49.97663640975952 loss, CER: 0.9580181837081909, 0.7339046001434326 sec\n",
      "7 epoch: 49.06383752822876 loss, CER: 0.968400239944458, 0.4747483730316162 sec\n",
      "8 epoch: 48.33909487724304 loss, CER: 0.9650269150733948, 0.4532613754272461 sec\n",
      "9 epoch: 47.74324631690979 loss, CER: 0.9638643860816956, 0.4011263847351074 sec\n",
      "10 epoch: 47.006303787231445 loss, CER: 0.9712439179420471, 0.5733516216278076 sec\n",
      "11 epoch: 45.912123680114746 loss, CER: 0.9666744470596313, 0.6571249961853027 sec\n",
      "12 epoch: 44.13912796974182 loss, CER: 0.9660217761993408, 0.4768686294555664 sec\n",
      "13 epoch: 41.76579213142395 loss, CER: 0.9608508944511414, 0.4630410671234131 sec\n",
      "14 epoch: 38.4572172164917 loss, CER: 0.8899868726730347, 0.42584228515625 sec\n",
      "15 epoch: 34.1526153087616 loss, CER: 0.68504798412323, 0.8149118423461914 sec\n",
      "16 epoch: 29.67371654510498 loss, CER: 0.5326887369155884, 0.46199536323547363 sec\n",
      "17 epoch: 25.74328851699829 loss, CER: 0.458545982837677, 0.48085784912109375 sec\n",
      "18 epoch: 22.442995429039 loss, CER: 0.41545119881629944, 0.5665016174316406 sec\n",
      "19 epoch: 19.73031198978424 loss, CER: 0.4022533595561981, 0.45804858207702637 sec\n",
      "20 epoch: 17.24470841884613 loss, CER: 0.37292012572288513, 0.7251336574554443 sec\n",
      "21 epoch: 14.780290305614471 loss, CER: 0.3145585358142853, 0.41668152809143066 sec\n",
      "22 epoch: 12.486814618110657 loss, CER: 0.24742253124713898, 0.6248915195465088 sec\n",
      "23 epoch: 10.386079013347626 loss, CER: 0.1882801502943039, 0.5498833656311035 sec\n",
      "24 epoch: 8.36988964676857 loss, CER: 0.15773358941078186, 0.9715394973754883 sec\n",
      "25 epoch: 6.732611060142517 loss, CER: 0.11034177243709564, 0.6340389251708984 sec\n",
      "26 epoch: 5.409282922744751 loss, CER: 0.09475582838058472, 0.47786808013916016 sec\n",
      "27 epoch: 4.404649585485458 loss, CER: 0.10063119232654572, 0.6615822315216064 sec\n",
      "28 epoch: 3.644699051976204 loss, CER: 0.10189230740070343, 0.6352977752685547 sec\n",
      "29 epoch: 3.089411050081253 loss, CER: 0.10017968714237213, 0.4767005443572998 sec\n",
      "30 epoch: 2.682483173906803 loss, CER: 0.10131090879440308, 0.4447801113128662 sec\n",
      "31 epoch: 2.3742355406284332 loss, CER: 0.102507084608078, 0.5278403759002686 sec\n",
      "32 epoch: 2.132408484816551 loss, CER: 0.10135501623153687, 0.7808258533477783 sec\n",
      "33 epoch: 1.950100600719452 loss, CER: 0.10020294785499573, 0.535550594329834 sec\n",
      "34 epoch: 1.8177934065461159 loss, CER: 0.09791402518749237, 0.976489782333374 sec\n",
      "35 epoch: 1.6779415979981422 loss, CER: 0.09960576891899109, 0.8294787406921387 sec\n",
      "36 epoch: 1.5450495332479477 loss, CER: 0.09791656583547592, 1.3747687339782715 sec\n",
      "37 epoch: 1.4549579247832298 loss, CER: 0.09960576891899109, 0.9412229061126709 sec\n",
      "38 epoch: 1.3626133017241955 loss, CER: 0.10018179565668106, 0.5330321788787842 sec\n",
      "39 epoch: 1.2867452055215836 loss, CER: 0.09963182359933853, 0.684983491897583 sec\n",
      "40 epoch: 1.2241667956113815 loss, CER: 0.09962127357721329, 0.6334426403045654 sec\n",
      "41 epoch: 1.1622489504516125 loss, CER: 0.10244956612586975, 0.5222909450531006 sec\n",
      "42 epoch: 1.103058561682701 loss, CER: 0.10131557285785675, 0.9524722099304199 sec\n",
      "43 epoch: 1.0493043698370457 loss, CER: 0.1013338714838028, 0.5080547332763672 sec\n",
      "44 epoch: 0.9959277100861073 loss, CER: 0.1030413955450058, 0.6931891441345215 sec\n",
      "45 epoch: 0.9624952599406242 loss, CER: 0.10188396275043488, 0.8111698627471924 sec\n",
      "46 epoch: 0.9163144417107105 loss, CER: 0.10303378105163574, 0.5967891216278076 sec\n",
      "47 epoch: 0.8813193440437317 loss, CER: 0.10187099128961563, 0.6197478771209717 sec\n",
      "48 epoch: 0.850399449467659 loss, CER: 0.09957719594240189, 0.5499515533447266 sec\n",
      "49 epoch: 0.8195101581513882 loss, CER: 0.10359520465135574, 0.46573543548583984 sec\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "import time\n",
    "\n",
    "model = Model(input_length=40, subsampled_input_length=32, num_labels=num_labels).to(DEVICE)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(reduction=\"sum\", blank=dataset.blank_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# Adam\n",
    "\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    cnt = 0\n",
    "    total_cer = 0\n",
    "    for _, (padded_spectrogram_dbs, padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(dataloader):\n",
    "        cnt += 1\n",
    "        optimizer.zero_grad()\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(DEVICE)\n",
    "        padded_text_idxs = padded_text_idxs.to(DEVICE)\n",
    "      \n",
    "        padded_log_probs, sub_sampled_padded_spectrogram_db_lens = model(padded_spectrogram_dbs, original_spectrofram_db_lens)\n",
    "        loss = ctc_loss(padded_log_probs.transpose(1,0), padded_text_idxs, sub_sampled_padded_spectrogram_db_lens, original_text_idx_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lossはバッチ内平均ロス\n",
    "        epoch_loss += (loss.item() / BATCH_SIZE)\n",
    "\n",
    "        hypotheses_idxs = padded_log_probs.argmax(dim=2) \n",
    "        hypotheses = ctc_simple_decode(hypotheses_idxs, idx_to_token=idx_to_token, padding_idx=pad_idx, blank_idx=blank_idx)\n",
    "        teachers = ctc_simple_decode(padded_text_idxs, idx_to_token=idx_to_token, padding_idx=pad_idx, blank_idx=blank_idx)\n",
    "        total_cer += char_error_rate(hypotheses, teachers)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"{i} epoch: {epoch_loss / cnt} loss, CER: {total_cer / cnt}, {t1 - t0} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
