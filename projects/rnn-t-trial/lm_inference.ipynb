{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lm_model import LSTMLM\n",
    "from tokenizer import SentencePieceTokenizer\n",
    "from data import LibriSpeechTextDataset, get_text_dataloader\n",
    "import torch\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_MODEL_FILE_PATH = \"./vocabs/librispeech_1024_bpe.model\"\n",
    "DATASET_JSON_FILE_PATH = \"./json/librispeech_train-clean-100.json\"\n",
    "LM_MODEL_FILE_PATH = \"./artifacts/librispeech-clean-100/6c74f0e19278402b87c4e21a4866e206/artifacts/model_34.pth\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceTokenizer(\n",
    "    model_file_path=TOKENIZER_MODEL_FILE_PATH\n",
    ")\n",
    "dataset = LibriSpeechTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    json_file_path=DATASET_JSON_FILE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(LM_MODEL_FILE_PATH, \"rb\") as f:\n",
    "    cpt = torch.load(f)\n",
    "model_state = cpt[\"model\"]\n",
    "model_args = cpt[\"model_args\"]\n",
    "language_model = LSTMLM(**model_args).to(DEVICE)\n",
    "language_model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 30\n",
    "prompt = dataset[IDX][1][:20].tolist()\n",
    "prompt = [tokenizer.bos_token_id] + prompt\n",
    "\n",
    "prompt = torch.tensor(prompt).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTMLM' object has no attribute 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m hyp_tokens \u001b[39m=\u001b[39m prompt[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m next_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m output, hidden \u001b[39m=\u001b[39m language_model\u001b[39m.\u001b[39;49minference(prompt, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m      6\u001b[0m next_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1176\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTMLM' object has no attribute 'inference'"
     ]
    }
   ],
   "source": [
    "# greedy decode\n",
    "hyp_tokens = prompt[0].tolist()\n",
    "next_token = None\n",
    "output, hidden = language_model.inference(prompt, None)\n",
    "output = output[0, -1, :]\n",
    "next_token = torch.argmax(output, dim=-1)\n",
    "hyp_tokens.append(next_token.item())\n",
    "while next_token != tokenizer.eos_token_id:\n",
    "    output, hidden = language_model.inference(next_token, hidden) #[1, T, D]\n",
    "    output =  output[0, -1, :]\n",
    "    next_token = torch.argmax(output, dim=-1)\n",
    "    hyp_tokens.append(next_token.item())\n",
    "    print(tokenizer.token_ids_to_text(hyp_tokens), end=\"\\r\")\n",
    "answer_tokens = dataset[IDX][1].tolist()\n",
    "answer = tokenizer.token_ids_to_text(answer_tokens)\n",
    "hyp = tokenizer.token_ids_to_text(hyp_tokens)\n",
    "print(f\"answer: {answer}\")\n",
    "print(f\"hypothesis: {hyp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypotheis:\n",
    "    def __init__(self, hyp: List[int], next_input: torch.Tensor, hidden, score):\n",
    "        self.hyp = hyp\n",
    "        self.next_input = next_input\n",
    "        self.hidden = hidden\n",
    "        self.score = score\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(\n",
    "            self,\n",
    "            beam_size: int,\n",
    "            max_length: int,\n",
    "            scorer,\n",
    "    ):\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        self.scorer = scorer\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            prompt: torch.Tensor,\n",
    "    ):\n",
    "        # prompt: [T]\n",
    "        initial_hypothesis = Hypotheis(prompt.tolist(), prompt, None, 0)\n",
    "        hypotheses = [initial_hypothesis]\n",
    "        next_hypotheses = []\n",
    "        length = prompt.shape[0]\n",
    "        ended_hypotheses = []\n",
    "        while length < self.max_length:\n",
    "            for hypothesis in hypotheses:\n",
    "                hyp, next_input, hidden, score = hypothesis.hyp, hypothesis.next_input, hypothesis.hidden, hypothesis.score\n",
    "                output, hidden = self.scorer.score(next_input, hidden) # [1, T, num_tokens]\n",
    "                output = output[0, -1, :]\n",
    "                topk = torch.topk(output, self.beam_size)\n",
    "                for i in range(self.beam_size):\n",
    "                    new_next_input = topk.indices[i]\n",
    "                    new_hyp = hyp + [new_next_input.item()]\n",
    "                    new_score = score + topk.values[i].item()\n",
    "                    new_hypothesis = Hypotheis(new_hyp, new_next_input, hidden, new_score)\n",
    "                    next_hypotheses.append(new_hypothesis)\n",
    "            next_hypotheses = sorted(next_hypotheses, key=lambda x: x.score, reverse=True)[:min(self.beam_size, len(next_hypotheses))]\n",
    "            if len(next_hypotheses) == 0:\n",
    "                break\n",
    "            print(f\"length: {length}, {tokenizer.token_ids_to_text(next_hypotheses[0].hyp)}\", end=\"\\r\")\n",
    "            next_hypotheses, ended_hypotheses = self.post_process(next_hypotheses, ended_hypotheses)\n",
    "            hypotheses = next_hypotheses\n",
    "            next_hypotheses = []\n",
    "            length += 1\n",
    "        \n",
    "        nbest_hypotheses = sorted(ended_hypotheses, key=lambda x: x.score, reverse=True)[:min(self.beam_size, len(ended_hypotheses))]\n",
    "        return nbest_hypotheses\n",
    "    \n",
    "    def post_process(self, next_hypotheses, ended_hypotheses):\n",
    "        remained_next_hypotheses = []\n",
    "        for hypothesis in next_hypotheses:\n",
    "            if hypothesis.next_input == tokenizer.eos_token_id:\n",
    "                ended_hypotheses.append(hypothesis)\n",
    "            else:\n",
    "                remained_next_hypotheses.append(hypothesis)\n",
    "        return remained_next_hypotheses, ended_hypotheses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 99, thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustine's husband's augusta's husband's husband's pennsylvania augustine'\r"
     ]
    }
   ],
   "source": [
    "beamsearch = BeamSearch(beam_size=10, max_length=100, scorer=language_model)\n",
    "ended_hyps = beamsearch.forward(prompt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thin woman with angles and without curves her dark hair showed some grays and scholarship: -0.5394296288490296\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's handwriting: -0.9109017410385423\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania: -0.8893814965353899\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustine: -0.9395778866985939\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augusta: -0.9620941595344062\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustlihood: -0.9829661327831056\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustliament: -0.9905155628765633\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustine was to be done: -1.0078941991253356\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustine's steps: -1.0024236040098913\n",
      "thin woman with angles and without curves her dark hair showed some grays and scholarship in the streets of the mountaineer's pennsylvania augustine's husband: -0.9903087728209077\n"
     ]
    }
   ],
   "source": [
    "for hyp in ended_hyps:\n",
    "    # normalized score\n",
    "    hyp.score /= len(hyp.hyp)\n",
    "    print(f\"{tokenizer.token_ids_to_text(hyp.hyp)}: {hyp.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
