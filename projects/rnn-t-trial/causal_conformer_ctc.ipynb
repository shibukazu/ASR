{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "import torch.nn\n",
    "from data import YesNoDataset\n",
    "from torchaudio.functional import rnnt_loss\n",
    "from torch.nn.functional import log_softmax\n",
    "from torchmetrics.functional import char_error_rate\n",
    "from modules.conformer.conformer import CausalConformerBlock\n",
    "from modules.subsampling import Conv2DSubSampling\n",
    "from modules.encoder import CausalConformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8\n",
    "dataset = YesNoDataset(\n",
    "    wav_dir_path=\"datasets/waves_yesno/\",\n",
    "    model_sample_rate=16000,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    drop_last=False,\n",
    ")\n",
    "num_labels = len(dataset.idx_to_token)\n",
    "token_to_idx = dataset.token_to_idx\n",
    "blank_idx = dataset.blank_idx\n",
    "pad_idx = dataset.pad_idx\n",
    "idx_to_token = dataset.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, subsampled_input_size, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.subsampled_input_size = subsampled_input_size\n",
    "        self.num_labels = num_labels\n",
    "        self.encoder = CausalConformerEncoder(\n",
    "            input_size=input_size,\n",
    "            subsampled_input_size=subsampled_input_size,\n",
    "            ff_hidden_size=32,\n",
    "            conv_hidden_size=64,\n",
    "            conv_kernel_size=8,\n",
    "            mha_num_heads=4,\n",
    "            dropout=0.1,\n",
    "            num_conformer_blocks=1,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(subsampled_input_size, num_labels)\n",
    "    \n",
    "    def forward(self, padded_input, input_lengths):\n",
    "        padded_output, subsampled_input_lengths = self.encoder(padded_input, input_lengths)\n",
    "        padded_output = self.fc(padded_output)\n",
    "        padded_log_prob = log_softmax(padded_output, dim=2)\n",
    "        return padded_log_prob, subsampled_input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_simple_decode(hypotheses_idxs, idx_to_token, padding_idx, blank_idx):\n",
    "\n",
    "    hypotheses_idxs = hypotheses_idxs.cpu().numpy()\n",
    "    hypotheses = []\n",
    "    for hypothesis_idxs in hypotheses_idxs:\n",
    "        hypothesis = []\n",
    "        prev_idx = -1\n",
    "        for idx in hypothesis_idxs:\n",
    "            if idx == blank_idx:\n",
    "                continue\n",
    "            elif idx == prev_idx:\n",
    "                continue\n",
    "            elif idx == padding_idx:\n",
    "                continue\n",
    "            else:\n",
    "                hypothesis.append(idx_to_token[idx])\n",
    "                prev_idx = idx\n",
    "        hypotheses.append(\"\".join(hypothesis))\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "0 epoch: 73.52474880218506 loss, CER: 0.8217387795448303, 6.232017993927002 sec\n",
      "epoch: 1\n",
      "1 epoch: 48.50467109680176 loss, CER: 0.9225143790245056, 7.184962511062622 sec\n",
      "epoch: 2\n",
      "2 epoch: 47.51719260215759 loss, CER: 0.9158633351325989, 7.084156036376953 sec\n",
      "epoch: 3\n",
      "3 epoch: 47.17652463912964 loss, CER: 0.8755994439125061, 7.802304029464722 sec\n",
      "epoch: 4\n",
      "4 epoch: 46.967416286468506 loss, CER: 0.885417103767395, 6.042619943618774 sec\n",
      "epoch: 5\n",
      "5 epoch: 46.80734133720398 loss, CER: 0.8968003392219543, 5.90822958946228 sec\n",
      "epoch: 6\n",
      "6 epoch: 46.67368817329407 loss, CER: 0.9079841375350952, 5.460905313491821 sec\n",
      "epoch: 7\n",
      "7 epoch: 46.54531240463257 loss, CER: 0.9292165040969849, 6.78022313117981 sec\n",
      "epoch: 8\n",
      "8 epoch: 46.336510181427 loss, CER: 0.9083645343780518, 6.57177734375 sec\n",
      "epoch: 9\n",
      "9 epoch: 45.97253894805908 loss, CER: 0.9038147926330566, 6.470540523529053 sec\n",
      "epoch: 10\n",
      "10 epoch: 45.085493087768555 loss, CER: 0.925201952457428, 6.819506406784058 sec\n",
      "epoch: 11\n",
      "11 epoch: 42.80850529670715 loss, CER: 0.8517906665802002, 6.199695110321045 sec\n",
      "epoch: 12\n",
      "12 epoch: 38.68131184577942 loss, CER: 0.7159780859947205, 5.841424465179443 sec\n",
      "epoch: 13\n",
      "13 epoch: 33.08475303649902 loss, CER: 0.6322486400604248, 6.630826234817505 sec\n",
      "epoch: 14\n",
      "14 epoch: 28.20584225654602 loss, CER: 0.5114743709564209, 6.835510492324829 sec\n",
      "epoch: 15\n",
      "15 epoch: 24.298725366592407 loss, CER: 0.44755449891090393, 5.683010816574097 sec\n",
      "epoch: 16\n",
      "16 epoch: 20.7237765789032 loss, CER: 0.37947899103164673, 5.85477614402771 sec\n",
      "epoch: 17\n",
      "17 epoch: 16.610301673412323 loss, CER: 0.2659279704093933, 5.191611051559448 sec\n",
      "epoch: 18\n",
      "18 epoch: 12.65882408618927 loss, CER: 0.10682808607816696, 5.844454765319824 sec\n",
      "epoch: 19\n",
      "19 epoch: 9.366525411605835 loss, CER: 0.07725471258163452, 4.192894697189331 sec\n",
      "epoch: 20\n",
      "20 epoch: 7.173102170228958 loss, CER: 0.0974552258849144, 6.1882548332214355 sec\n",
      "epoch: 21\n",
      "21 epoch: 5.435628846287727 loss, CER: 0.12664224207401276, 6.074209690093994 sec\n",
      "epoch: 22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m total_cer \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfor\u001b[39;00m _, (padded_spectrogram_dbs, padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     19\u001b[0m     cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/n/work3/shibutani/ASR/projects/rnn-t-trial/data.py:65\u001b[0m, in \u001b[0;36mYesNoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m     wav_data \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mresample(wav_data, sample_rate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_sample_rate)\n\u001b[1;32m     64\u001b[0m     sample_rate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_sample_rate\n\u001b[0;32m---> 65\u001b[0m spectrogram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspectrogram_transformer(wav_data)\n\u001b[1;32m     66\u001b[0m spectrogram_db \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mamplitude_to_db(spectrogram)\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m spectrogram_db[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mtensor(text_idx, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint32)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torchaudio/transforms.py:588\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[39m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    587\u001b[0m specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspectrogram(waveform)\n\u001b[0;32m--> 588\u001b[0m mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmel_scale(specgram)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torchaudio/transforms.py:384\u001b[0m, in \u001b[0;36mMelScale.forward\u001b[0;34m(self, specgram)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39m    specgram (Tensor): A spectrogram STFT of dimension (..., freq, time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# (..., time, freq) dot (freq, n_mels) -> (..., n_mels, time)\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m mel_specgram \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(specgram\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfb)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "import time\n",
    "\n",
    "model = Model(input_size=40, subsampled_input_size=32, num_labels=num_labels).to(DEVICE)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(reduction=\"sum\", blank=dataset.blank_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# Adam\n",
    "\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    cnt = 0\n",
    "    total_cer = 0\n",
    "    print(f\"epoch: {i}\")\n",
    "    for _, (padded_spectrogram_dbs, padded_text_idxs, original_spectrofram_db_lens, original_text_idx_lens) in enumerate(dataloader):\n",
    "        cnt += 1\n",
    "        optimizer.zero_grad()\n",
    "        padded_spectrogram_dbs = padded_spectrogram_dbs.to(DEVICE)\n",
    "        padded_text_idxs = padded_text_idxs.to(DEVICE)\n",
    "      \n",
    "        padded_log_probs, sub_sampled_padded_spectrogram_db_lens = model(padded_spectrogram_dbs, original_spectrofram_db_lens)\n",
    "        loss = ctc_loss(padded_log_probs.transpose(1,0), padded_text_idxs, sub_sampled_padded_spectrogram_db_lens, original_text_idx_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lossはバッチ内平均ロス\n",
    "        epoch_loss += (loss.item() / BATCH_SIZE)\n",
    "\n",
    "        hypotheses_idxs = padded_log_probs.argmax(dim=2) \n",
    "        hypotheses = ctc_simple_decode(hypotheses_idxs, idx_to_token=idx_to_token, padding_idx=pad_idx, blank_idx=blank_idx)\n",
    "        teachers = ctc_simple_decode(padded_text_idxs, idx_to_token=idx_to_token, padding_idx=pad_idx, blank_idx=blank_idx)\n",
    "        total_cer += char_error_rate(hypotheses, teachers)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"{i} epoch: {epoch_loss / cnt} loss, CER: {total_cer / cnt}, {t1 - t0} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
