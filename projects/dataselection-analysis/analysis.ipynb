{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from model import MyWav2Vec2ConformerForPreTraining\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List\n",
    "from gruut import sentences\n",
    "import gruut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"brighter than early dawn's most brilliant dye are blown clear bands of color through the sky that swirl and sweep and meet to break and foam like rainbow veils upon a bubble's dome\"\n",
    "phoneme_list1 = []\n",
    "for sent in sentences(text, lang=\"en-us\"):\n",
    "    for word in sent:\n",
    "        if word.phonemes:\n",
    "            for phoneme in word.phonemes:\n",
    "                phoneme_list1.append(phoneme.lstrip(\"ˈ\"))\n",
    "text = \"in a sunset glowing of crimson and gold she lies the glory of the world a beached king's galley whose sails are furled who is hung with tapestries rich and old\"\n",
    "phoneme_list2 = []\n",
    "for sent in sentences(text, lang=\"en-us\"):\n",
    "    for word in sent:\n",
    "        if word.phonemes:\n",
    "            for phoneme in word.phonemes:\n",
    "                phoneme_list2.append(phoneme.lstrip(\"ˈ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyWav2Vec2ConformerForPreTraining.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = model.config.num_codevector_groups\n",
    "V = model.config.num_codevectors_per_group\n",
    "max_index = G * V\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_kl_divergence(indices: List[np.ndarray], max_index: int) -> float:\n",
    "    \"\"\"\n",
    "    indices: (all_data_size, num_codebooks * seq_len)\n",
    "            all_data_size: 同一の発話（マイク）のデータ数\n",
    "            同一の発話内容（マイク）におけるすべてのインデックス系列\n",
    "    同一の発話内容(マイク)におけるすべてのインデックス系列の平均KLダイバージェンスを計算する\n",
    "    \"\"\"\n",
    "    total_number = len(indices) * len(indices)\n",
    "    counter = 0\n",
    "    eps = 1e-8\n",
    "    bins = np.linspace(0, max_index, max_index + 1)\n",
    "    average_kl_divergence = 0\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices)):\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0:\n",
    "                print(f\"progress: {counter}, {counter / total_number * 100:.2f}%\")\n",
    "            hist1, bin_edges1 = np.histogram(indices[i], bins=bins, density=True)\n",
    "            hist1 += eps\n",
    "            hist1 = hist1 / (np.diff(bin_edges1) * hist1.sum())\n",
    "            hist2, bin_edges2 = np.histogram(indices[j], bins=bins, density=True)\n",
    "            hist2 += eps\n",
    "            hist2 = hist2 / (np.diff(bin_edges2) * hist2.sum())\n",
    "\n",
    "            average_kl_divergence += np.sum(hist1 * np.log(hist1 / hist2))\n",
    "            if i == j:\n",
    "                assert np.abs(np.sum(hist1 * np.log(hist1 / hist2)) - 0) < eps\n",
    "\n",
    "    average_kl_divergence /= total_number\n",
    "    return average_kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-100の1/50のキーをランダムサンプリング\n",
    "# (nexus6がいくつかのquantizeに失敗しているため、nexus6のキーからサンプリング)\n",
    "# これらのキーに対してのみKLダイバージェンスを計算する\n",
    "f_name = f\"pickles/nexus6_quantized_indices.pkl\"\n",
    "with open(f_name, \"rb\") as f:\n",
    "    matrix_quantized_indices = pickle.load(f)\n",
    "sampled_keys = np.random.choice(list(matrix_quantized_indices.keys()), size=len(matrix_quantized_indices) // 100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mic: matrix\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 1.4124794345111977\n",
      "mic: nexus6\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 0.9872670687122076\n",
      "mic: pseye\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 1.3830109286163514\n",
      "mic: respeaker\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 1.177017115959084\n",
      "mic: shure\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 1.1112444849123906\n",
      "mic: usb\n",
      "progress: 10000, 18.58%\n",
      "progress: 20000, 37.16%\n",
      "progress: 30000, 55.74%\n",
      "progress: 40000, 74.32%\n",
      "progress: 50000, 92.90%\n",
      "kl_divergence: 1.2632468073121972\n",
      "average kl_divergence: 1.2223776400039048\n"
     ]
    }
   ],
   "source": [
    "# 同一のマイク内での平均KL距離\n",
    "mic_names = [\"matrix\", \"nexus6\", \"pseye\", \"respeaker\", \"shure\", \"usb\"]\n",
    "mic_kl_divergences = {}\n",
    "for mic_name in mic_names:\n",
    "    print(f\"mic: {mic_name}\")\n",
    "    f_name = f\"pickles/{mic_name}_quantized_indices.pkl\"\n",
    "    with open(f_name, \"rb\") as f:\n",
    "        quantized_indices = pickle.load(f)\n",
    "\n",
    "    selected_quantized_indices = []\n",
    "    for key in sampled_keys:\n",
    "        selected_quantized_indices.append(quantized_indices[key])\n",
    "    kl_divergence = calculate_average_kl_divergence(selected_quantized_indices, max_index)\n",
    "    mic_kl_divergences[mic_name] = kl_divergence\n",
    "    print(f\"kl_divergence: {kl_divergence}\")\n",
    "\n",
    "# 全体の平均KL距離\n",
    "print(f\"average kl_divergence: {np.mean(list(mic_kl_divergences.values()))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterance_key: 4014-186179-0009.wav\n",
      "progress: 0, 0.00%\n",
      "kl_divergence: 0.24982588351576154\n",
      "utterance_key: 3982-182255-0039.wav\n",
      "progress: 1, 0.43%\n",
      "kl_divergence: 0.5920018075510902\n",
      "utterance_key: 4051-11218-0007.wav\n",
      "progress: 2, 0.86%\n",
      "kl_divergence: 0.3423940646548176\n",
      "utterance_key: 3879-174923-0022.wav\n",
      "progress: 3, 1.29%\n",
      "kl_divergence: 0.3104055583548819\n",
      "utterance_key: 6836-76549-0014.wav\n",
      "progress: 4, 1.72%\n",
      "kl_divergence: 0.4934494701185529\n",
      "utterance_key: 5688-41232-0034.wav\n",
      "progress: 5, 2.16%\n",
      "kl_divergence: 0.4209460848989326\n",
      "utterance_key: 4441-76250-0022.wav\n",
      "progress: 6, 2.59%\n",
      "kl_divergence: 0.46592952371861124\n",
      "utterance_key: 7447-91187-0020.wav\n",
      "progress: 7, 3.02%\n",
      "kl_divergence: 0.6523703903022311\n",
      "utterance_key: 89-219-0041.wav\n",
      "progress: 8, 3.45%\n",
      "kl_divergence: 0.3554165077674751\n",
      "utterance_key: 2289-152254-0002.wav\n",
      "progress: 9, 3.88%\n",
      "kl_divergence: 0.3558718505124323\n",
      "utterance_key: 3214-167606-0010.wav\n",
      "progress: 10, 4.31%\n",
      "kl_divergence: 0.24218741239991365\n",
      "utterance_key: 441-130108-0036.wav\n",
      "progress: 11, 4.74%\n",
      "kl_divergence: 0.593709867582965\n",
      "utterance_key: 7635-105409-0071.wav\n",
      "progress: 12, 5.17%\n",
      "kl_divergence: 0.3917862358810342\n",
      "utterance_key: 1455-138263-0003.wav\n",
      "progress: 13, 5.60%\n",
      "kl_divergence: 0.5060747478674492\n",
      "utterance_key: 3374-298032-0039.wav\n",
      "progress: 14, 6.03%\n",
      "kl_divergence: 0.1961485761875117\n",
      "utterance_key: 4481-17499-0039.wav\n",
      "progress: 15, 6.47%\n",
      "kl_divergence: 0.3773021678470125\n",
      "utterance_key: 254-127705-0035.wav\n",
      "progress: 16, 6.90%\n",
      "kl_divergence: 0.5337759630771308\n",
      "utterance_key: 19-198-0030.wav\n",
      "progress: 17, 7.33%\n",
      "kl_divergence: 0.45352640471815775\n",
      "utterance_key: 5703-47198-0019.wav\n",
      "progress: 18, 7.76%\n",
      "kl_divergence: 0.5304893402624523\n",
      "utterance_key: 911-130578-0018.wav\n",
      "progress: 19, 8.19%\n",
      "kl_divergence: 0.34881964360823564\n",
      "utterance_key: 229-130880-0014.wav\n",
      "progress: 20, 8.62%\n"
     ]
    }
   ],
   "source": [
    "# 同一の発話内での平均KL距離\n",
    "utterance_kl_divergences = {}\n",
    "for idx, utterance_key in enumerate(sampled_keys):\n",
    "    print(f\"utterance_key: {utterance_key}\")\n",
    "    print(f\"progress: {idx}, {idx / len(sampled_keys) * 100:.2f}%\")\n",
    "    selected_quantized_indices = []\n",
    "    for mic_name in mic_names:\n",
    "        f_name = f\"pickles/{mic_name}_quantized_indices.pkl\"\n",
    "        with open(f_name, \"rb\") as f:\n",
    "            quantized_indices = pickle.load(f)\n",
    "        selected_quantized_indices.append(quantized_indices[utterance_key])\n",
    "    kl_divergence = calculate_average_kl_divergence(selected_quantized_indices, max_index)\n",
    "    utterance_kl_divergences[utterance_key] = kl_divergence\n",
    "    print(f\"kl_divergence: {kl_divergence}\")\n",
    "\n",
    "# 全体の平均KL距離\n",
    "print(f\"average kl_divergence: {np.mean(list(utterance_kl_divergences.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    cache_dir=\"/home/shibutani/fs/.cache/huggingface/transformers\")\n",
    "bert_model = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    cache_dir=\"/home/shibutani/fs/.cache/huggingface/transformers\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "t2 = \"You're a graduate student at the University of Tokyo.\"\n",
    "#t3 = \"I'm interested in speech recongnition using neural network and machine learning and natural language processing.\"\n",
    "t3 = \"You're a graduate student at the University of Tokyo.\"\n",
    "sentences = [t2, t3]\n",
    "encoded_input = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "input_ids = encoded_input[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = encoded_input[\"attention_mask\"].to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "last_hidden_states = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各文章内で分散表現の和を計算 (文章の長さで正規化)\n",
    "sentence_embed_vecs = (last_hidden_states * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.unsqueeze(-1).sum(dim=1)\n",
    "cos_similarity = torch.nn.functional.cosine_similarity(\n",
    "                sentence_embed_vecs[0], sentence_embed_vecs[1], dim=0)\n",
    "cos_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
