{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TIMITDatasetMelSpecdb, get_dataloader\n",
    "import torch\n",
    "import torchaudio\n",
    "from model import Model\n",
    "from torchmetrics.functional import char_error_rate, word_error_rate\n",
    "from modules.decoders.ctc import greedy_decoder\n",
    "from quantizer import Quantizer\n",
    "from typing import Dict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7dc5a6ddcdc99305\n",
      "Found cached dataset timit (/home/shibutani/fs/.cache/huggingface/datasets/timit/default-7dc5a6ddcdc99305/0.0.0/e393649805e8c068eb5c3311baf236f53ffa81289ecc57e285c6e06a31f00ba8)\n",
      "100%|██████████| 2/2 [00:00<00:00, 455.01it/s]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataset = TIMITDatasetMelSpecdb(\"test\")\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False, drop_last=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4620"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"quantized_indices_memory.pkl\", \"rb\") as f:\n",
    "    quantized_indices_memory = pickle.load(f)\n",
    "len(quantized_indices_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantizer = Quantizer(DEVICE)\n",
    "#quantized_indices_memory = {}\n",
    "#for idx in range(len(train_dataset)):\n",
    "#    # show progress\n",
    "#    if idx % 100 == 0:\n",
    "#        print(f\"{idx / len(train_dataset) * 100:.2f}%\")\n",
    "#    audio = train_dataset[idx][-1].unsqueeze(0).to(DEVICE)\n",
    "#    quantized_indices = quantizer.quantize(audio)\n",
    "#    quantized_indices_memory[idx] = quantized_indices[0].tolist()\n",
    "#with open(\"quantized_indices_memory.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(quantized_indices_memory, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_divergence_between_uniform_and_empirical_distribution(sampled_quantized_idx_count: torch.Tensor):\n",
    "    assert sampled_quantized_idx_count.ndim == 1\n",
    "    # update sampled_quantized_idx_count\n",
    "    # calculate KL divergence\n",
    "    empirical_distribution = sampled_quantized_idx_count / sampled_quantized_idx_count.sum()\n",
    "    uniform_distribution = torch.ones_like(empirical_distribution) / len(empirical_distribution)\n",
    "    kl_divergence = torch.sum(empirical_distribution * torch.log(empirical_distribution / uniform_distribution))\n",
    "    if kl_divergence < 0:\n",
    "        raise ValueError(\"KL divergence must be positive.\")\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1952)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_quantized_idx_count = torch.randint(0, 200, (320*320,), dtype=torch.float32)\n",
    "sampled_quantized_idx_count += 1e-8\n",
    "calculate_kl_divergence_between_uniform_and_empirical_distribution(sampled_quantized_idx_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLBasedSampler:\n",
    "    def __init__(self, quantized_indices_memory: Dict, dataset: torch.utils.data.Dataset, ratio: float, device: torch.device):\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.ratio = ratio\n",
    "        self.target_num_samples = int(len(dataset) * ratio)\n",
    "        self.initial_num_samples =int(0.1 * self.target_num_samples)\n",
    "        self.sampled_indices = set()\n",
    "        self.not_sampled_indices = set(range(len(dataset)))\n",
    "        self.sampled_quantized_idx_count = torch.zeros(320 * 320, dtype=torch.float32)\n",
    "        self.quantized_indices_memory = quantized_indices_memory\n",
    "        # sample initial samples\n",
    "        self.sample_initial_samples()\n",
    "    \n",
    "    def calculate_kl_divergence_between_uniform_and_empirical_distribution(self, sample):\n",
    "        \"\"\"Calculate KL divergence between uniform and empirical distribution.\n",
    "        Args:\n",
    "            sample (1D torch.Tensor): quantized indices tensor\n",
    "        Returns:\n",
    "            kl_divergence (float): KL divergence between uniform and empirical distribution.\n",
    "        \"\"\"\n",
    "        assert sample.ndim == 1\n",
    "        # update sampled_quantized_idx_count\n",
    "        sampled_quantized_idx_count = self.sampled_quantized_idx_count.clone()\n",
    "        with torch.no_grad():\n",
    "            for quantized_idx in sample:\n",
    "                sampled_quantized_idx_count[quantized_idx] += 1\n",
    "        # calculate KL divergence\n",
    "        # avoid zero division\n",
    "        sampled_quantized_idx_count += 1e-8\n",
    "        empirical_distribution = sampled_quantized_idx_count / sampled_quantized_idx_count.sum()\n",
    "        uniform_distribution = torch.ones_like(empirical_distribution) / len(empirical_distribution)\n",
    "        kl_divergence = torch.sum(empirical_distribution * torch.log(empirical_distribution / uniform_distribution))\n",
    "        if kl_divergence < 0:\n",
    "            raise ValueError(\"KL divergence must be positive.\")\n",
    "        return kl_divergence\n",
    "\n",
    "    def sample_initial_samples(self):\n",
    "        # sample initial samples based on random sampling\n",
    "        initial_indices = torch.randperm(len(self.dataset))[:self.initial_num_samples].tolist()\n",
    "        self.sampled_indices.update(initial_indices)\n",
    "        self.not_sampled_indices.difference_update(initial_indices)\n",
    "        # update sampled_quantized_idx_count\n",
    "        with torch.no_grad():\n",
    "            for idx in initial_indices:\n",
    "                quantized_indices = torch.tensor(self.quantized_indices_memory[idx])\n",
    "                for quantized_idx in quantized_indices:\n",
    "                    self.sampled_quantized_idx_count[quantized_idx] += 1\n",
    "\n",
    "    def sample(self):\n",
    "        # sample new samples based on KL divergence until the number of samples reaches the target number of samples\n",
    "        # return subset of dataset\n",
    "        while len(self.sampled_indices) < self.target_num_samples:\n",
    "            # show progress\n",
    "            print(f\"sampled {len(self.sampled_indices) / self.target_num_samples * 100:.2f} %\")\n",
    "            # calculate KL divergence for each not sampled indices\n",
    "            kl_divergences = {}\n",
    "            not_sampled_indices = list(self.not_sampled_indices)\n",
    "            for idx in not_sampled_indices:\n",
    "                quantized_indices = torch.tensor(self.quantized_indices_memory[idx])\n",
    "                kl_divergence = self.calculate_kl_divergence_between_uniform_and_empirical_distribution(quantized_indices)\n",
    "                kl_divergences[idx] = kl_divergence\n",
    "            # select the index with the minimum KL divergence\n",
    "            min_kl_divergence_idx = min(kl_divergences.keys(), key=kl_divergences.get)\n",
    "            print(f\"min_kl_divergence: {kl_divergences[min_kl_divergence_idx]}\")\n",
    "            # update sampled_indices and not_sampled_indices\n",
    "            self.sampled_indices.add(min_kl_divergence_idx)\n",
    "            self.not_sampled_indices.remove(min_kl_divergence_idx)\n",
    "            # update sampled_quantized_idx_count\n",
    "            quantized_indices = self.quantized_indices_memory[min_kl_divergence_idx]\n",
    "            for quantized_idx in quantized_indices:\n",
    "                self.sampled_quantized_idx_count[quantized_idx] += 1\n",
    "\n",
    "        # return subset of self.dataset\n",
    "        return torch.utils.data.Subset(self.dataset, list(self.sampled_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7dc5a6ddcdc99305\n",
      "Found cached dataset timit (/home/shibutani/fs/.cache/huggingface/datasets/timit/default-7dc5a6ddcdc99305/0.0.0/e393649805e8c068eb5c3311baf236f53ffa81289ecc57e285c6e06a31f00ba8)\n",
      "100%|██████████| 2/2 [00:00<00:00, 495.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TIMITDatasetMelSpecdb(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 9.96 %\n",
      "min_kl_divergence: 6.10467004776001\n",
      "sampled 10.17 %\n",
      "min_kl_divergence: 6.0709309577941895\n",
      "sampled 10.39 %\n",
      "min_kl_divergence: 6.040905475616455\n",
      "sampled 10.61 %\n",
      "min_kl_divergence: 6.013134956359863\n",
      "sampled 10.82 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m kl_based_sampler \u001b[39m=\u001b[39m KLBasedSampler(quantized_indices_memory\u001b[39m=\u001b[39mquantized_indices_memory, dataset\u001b[39m=\u001b[39mtrain_dataset, ratio\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, device\u001b[39m=\u001b[39mDEVICE)\n\u001b[0;32m----> 2\u001b[0m subset \u001b[39m=\u001b[39m kl_based_sampler\u001b[39m.\u001b[39;49msample()\n",
      "Cell \u001b[0;32mIn [51], line 61\u001b[0m, in \u001b[0;36mKLBasedSampler.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m not_sampled_indices:\n\u001b[1;32m     60\u001b[0m     quantized_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantized_indices_memory[idx])\n\u001b[0;32m---> 61\u001b[0m     kl_divergence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_kl_divergence_between_uniform_and_empirical_distribution(quantized_indices)\n\u001b[1;32m     62\u001b[0m     kl_divergences[idx] \u001b[39m=\u001b[39m kl_divergence\n\u001b[1;32m     63\u001b[0m \u001b[39m# select the index with the minimum KL divergence\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [51], line 32\u001b[0m, in \u001b[0;36mKLBasedSampler.calculate_kl_divergence_between_uniform_and_empirical_distribution\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     30\u001b[0m sampled_quantized_idx_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-8\u001b[39m\n\u001b[1;32m     31\u001b[0m empirical_distribution \u001b[39m=\u001b[39m sampled_quantized_idx_count \u001b[39m/\u001b[39m sampled_quantized_idx_count\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> 32\u001b[0m uniform_distribution \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mones_like(empirical_distribution) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(empirical_distribution)\n\u001b[1;32m     33\u001b[0m kl_divergence \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(empirical_distribution \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mlog(empirical_distribution \u001b[39m/\u001b[39m uniform_distribution))\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m kl_divergence \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kl_based_sampler = KLBasedSampler(quantized_indices_memory=quantized_indices_memory, dataset=train_dataset, ratio=0.1, device=DEVICE)\n",
    "subset = kl_based_sampler.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save subset\n",
    "with open(\"subset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampler(dataset: torch.utils.data.Dataset, ratio: float):\n",
    "    \"\"\"Randomly sample a subset of the dataset\"\"\"\n",
    "    num_samples = int(len(dataset) * ratio)\n",
    "    indices = torch.randperm(len(dataset))[:num_samples].tolist()\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_subset = random_sampler(train_dataset, 0.8)\n",
    "random_train_dataloader = get_dataloader(random_train_subset, batch_size=32, shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Learning is running on cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This Learning is running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = 80\n",
    "self_attn_dim = 256\n",
    "feed_forward_dim = 1024\n",
    "token_size = len(train_dataset.vocab.keys())\n",
    "num_epochs = 40\n",
    "total_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing.subsampling import Conv2DSubSampling\n",
    "from modules.transformers.encoder import TransformerEncoder\n",
    "from modules.transformers.scheduler import TransformerLR\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, nlabel):\n",
    "        super(Model, self).__init__()\n",
    "        self.nlabel = nlabel\n",
    "        # out_sizeがSelf-Attentionの入力次元\n",
    "        self.conv2d_sub_sampling = Conv2DSubSampling(\n",
    "            in_size=80, out_size=256, kernel_size1=3, kernel_size2=3, stride1=2, stride2=1)\n",
    "        # in_hidden_sizeがFFの次元\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            in_size=256, nlayer=12, nhead=4, in_hidden_size=1024, dropout=0.1, norm_first=True)\n",
    "        self.fc = nn.Linear(256, nlabel, bias=True)\n",
    "        self.log_softmax = nn.functional.log_softmax\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T, in_size]\n",
    "        #   x_lengths: [B]\n",
    "        #       padding前のシーケンス長\n",
    "        # return:\n",
    "        #   log_prob: [B, T, nlabel]\n",
    "        #   y_lengths: [B]\n",
    "        #       非パディング部分のシーケンス長\n",
    "        subsampled_x, subsampled_x_length = self.conv2d_sub_sampling(x, x_lengths)\n",
    "        encoded, encoded_inner = self.transformer_encoder(\n",
    "            subsampled_x, subsampled_x_length)  # [B, T', subsampled_in_size]\n",
    "        y = self.fc(encoded)  # [B, T', nlabel]\n",
    "        y_lengths = subsampled_x_length\n",
    "        log_probs = self.log_softmax(y, dim=-1)  # [B, T', nlabel]\n",
    "        return log_probs, y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 40 train loss: 207.77212298849355, train CER: 0.9964577555656433, train WER: 1.0\n",
      "Epoch 1 of 40 test loss: 156.152664472472, test CER: 1.0, test WER: 1.0\n",
      "Epoch 2 of 40 train loss: 153.84124742590862, train CER: 1.0, train WER: 1.0\n",
      "Epoch 2 of 40 test loss: 151.7391800790463, test CER: 1.0, test WER: 1.0\n",
      "Epoch 3 of 40 train loss: 150.87309464164403, train CER: 1.0, train WER: 1.0\n",
      "Epoch 3 of 40 test loss: 149.18733704764887, test CER: 1.0, test WER: 1.0\n",
      "Epoch 4 of 40 train loss: 143.1272523962933, train CER: 0.9989821314811707, train WER: 1.0\n",
      "Epoch 4 of 40 test loss: 133.57229225590544, test CER: 0.980032205581665, test WER: 1.0\n",
      "Epoch 5 of 40 train loss: 124.56327003810716, train CER: 0.8546525835990906, train WER: 1.0004898309707642\n",
      "Epoch 5 of 40 test loss: 114.21900853570902, test CER: 0.7276081442832947, test WER: 0.9961405396461487\n",
      "Epoch 6 of 40 train loss: 107.17956980829653, train CER: 0.676473081111908, train WER: 0.992084264755249\n",
      "Epoch 6 of 40 test loss: 98.6155336487968, test CER: 0.634667694568634, test WER: 0.9729825854301453\n",
      "Epoch 7 of 40 train loss: 95.14691878609035, train CER: 0.604808509349823, train WER: 0.9589177966117859\n",
      "Epoch 7 of 40 test loss: 89.90333758660083, test CER: 0.5672167539596558, test WER: 0.9248512983322144\n",
      "Epoch 8 of 40 train loss: 87.41515947424847, train CER: 0.5632120966911316, train WER: 0.9174705147743225\n",
      "Epoch 8 of 40 test loss: 85.02879880509286, test CER: 0.5320789217948914, test WER: 0.9048072695732117\n",
      "Epoch 9 of 40 train loss: 81.98691004877504, train CER: 0.5595496296882629, train WER: 0.8903186321258545\n",
      "Epoch 9 of 40 test loss: 81.70076766104069, test CER: 0.5135196447372437, test WER: 0.891610860824585\n",
      "Epoch 10 of 40 train loss: 78.32370233950408, train CER: 0.5127531886100769, train WER: 0.8741864562034607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m by_len \u001b[39m=\u001b[39m by_len\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     48\u001b[0m log_probs, y_lengths \u001b[39m=\u001b[39m asr_model(bx, bx_len)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[39m=\u001b[39m ctc_loss(log_probs\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m), by, y_lengths, by_len)\n\u001b[1;32m     50\u001b[0m test_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m bx\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     52\u001b[0m \u001b[39m# calculate CER\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/loss.py:1714\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mctc_loss(log_probs, targets, input_lengths, target_lengths, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblank, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1715\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_infinity)\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/functional.py:2460\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2454\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2455\u001b[0m         ctc_loss,\n\u001b[1;32m   2456\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2457\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2458\u001b[0m         blank\u001b[39m=\u001b[39mblank, reduction\u001b[39m=\u001b[39mreduction, zero_infinity\u001b[39m=\u001b[39mzero_infinity\n\u001b[1;32m   2459\u001b[0m     )\n\u001b[0;32m-> 2460\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mctc_loss(\n\u001b[1;32m   2461\u001b[0m     log_probs, targets, input_lengths, target_lengths, blank, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), zero_infinity\n\u001b[1;32m   2462\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "asr_model = Model(nlabel=token_size).to(DEVICE)\n",
    "ctc_loss = torch.nn.CTCLoss(reduction=\"sum\", blank=train_dataset.ctc_token_id)\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = TransformerLR(optimizer, d_model=256, warmup_steps=8000) # Warmup終了時点でおよそ0.0017になっている\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    asr_model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_cer = 0\n",
    "    train_epoch_wer = 0\n",
    "    train_cnt = 0\n",
    "\n",
    "    for i, (bidx, bx, bx_len, by, by_len, texts, phonetic_details) in enumerate(random_train_dataloader):\n",
    "        bx = bx.to(DEVICE)\n",
    "        bx_len = bx_len.to(DEVICE)\n",
    "        by = by.to(DEVICE)\n",
    "        by_len = by_len.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs, y_lengths = asr_model(bx, bx_len)\n",
    "        loss = ctc_loss(log_probs.transpose(1, 0), by, y_lengths, by_len)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_epoch_loss += (loss.item() / bx.size(0))\n",
    "\n",
    "        # calculate CER\n",
    "        hypothesis = torch.argmax(log_probs, dim=-1)\n",
    "        hypotheses = greedy_decoder(hypothesis, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "        answers = greedy_decoder(by, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "        train_epoch_cer += char_error_rate(hypotheses, answers)\n",
    "        train_epoch_wer += word_error_rate(hypotheses, answers)\n",
    "\n",
    "        train_cnt += 1\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs} train loss: {train_epoch_loss / train_cnt}, train CER: {train_epoch_cer / train_cnt}, train WER: {train_epoch_wer / train_cnt}\")\n",
    "\n",
    "    asr_model.eval()\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_cer = 0\n",
    "    test_epoch_wer = 0\n",
    "    test_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (bidx, bx, bx_len, by, by_len, texts, phonetic_details) in enumerate(test_dataloader):\n",
    "            bx = bx.to(DEVICE)\n",
    "            bx_len = bx_len.to(DEVICE)\n",
    "            by = by.to(DEVICE)\n",
    "            by_len = by_len.to(DEVICE)\n",
    "            log_probs, y_lengths = asr_model(bx, bx_len)\n",
    "            loss = ctc_loss(log_probs.transpose(1, 0), by, y_lengths, by_len)\n",
    "            test_epoch_loss += (loss.item() / bx.size(0))\n",
    "\n",
    "            # calculate CER\n",
    "            hypothesis = torch.argmax(log_probs, dim=-1)\n",
    "            hypotheses = greedy_decoder(hypothesis, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "            answers = greedy_decoder(by, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "            test_epoch_cer += char_error_rate(hypotheses, answers)\n",
    "            test_epoch_wer += word_error_rate(hypotheses, answers)\n",
    "\n",
    "            test_cnt += 1\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs} test loss: {test_epoch_loss / test_cnt}, test CER: {test_epoch_cer / test_cnt}, test WER: {test_epoch_wer / test_cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
