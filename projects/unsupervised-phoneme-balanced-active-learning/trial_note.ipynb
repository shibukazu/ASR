{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shibutani/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import TIMITDatasetMelSpecdb, get_dataloader, LibriSpeechDataset\n",
    "import torch\n",
    "import torchaudio\n",
    "from model import Model\n",
    "from torchmetrics.functional import char_error_rate, word_error_rate\n",
    "from modules.decoders.ctc import greedy_decoder\n",
    "from quantizer import Quantizer\n",
    "from typing import Dict\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28539"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librispeech_train_dataset = LibriSpeechDataset(\n",
    "    root=\"./datasets\",\n",
    "    split=\"train\"\n",
    ")\n",
    "len(librispeech_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"librispeech_quantized_indices_memory.pkl\", \"rb\") as f:\n",
    "            quantized_indices_memory = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%\n",
      "0.35%\n",
      "0.70%\n",
      "1.05%\n",
      "1.40%\n",
      "1.75%\n",
      "2.10%\n",
      "2.45%\n",
      "2.80%\n",
      "3.15%\n",
      "3.50%\n",
      "3.85%\n",
      "4.20%\n",
      "4.56%\n",
      "4.91%\n",
      "5.26%\n",
      "5.61%\n",
      "5.96%\n",
      "6.31%\n",
      "6.66%\n",
      "7.01%\n",
      "7.36%\n",
      "7.71%\n",
      "8.06%\n",
      "8.41%\n",
      "8.76%\n",
      "9.11%\n",
      "9.46%\n",
      "9.81%\n",
      "10.16%\n",
      "10.51%\n",
      "10.86%\n",
      "11.21%\n",
      "11.56%\n",
      "11.91%\n",
      "12.26%\n",
      "12.61%\n",
      "12.96%\n",
      "13.32%\n",
      "13.67%\n",
      "14.02%\n",
      "14.37%\n",
      "14.72%\n",
      "15.07%\n",
      "15.42%\n",
      "15.77%\n",
      "16.12%\n",
      "16.47%\n",
      "16.82%\n",
      "17.17%\n",
      "17.52%\n",
      "17.87%\n",
      "18.22%\n",
      "18.57%\n",
      "18.92%\n",
      "19.27%\n",
      "19.62%\n",
      "19.97%\n",
      "20.32%\n",
      "20.67%\n",
      "21.02%\n",
      "21.37%\n",
      "21.72%\n",
      "22.08%\n",
      "22.43%\n",
      "22.78%\n",
      "23.13%\n",
      "23.48%\n",
      "23.83%\n",
      "24.18%\n",
      "24.53%\n",
      "24.88%\n",
      "25.23%\n",
      "25.58%\n",
      "25.93%\n",
      "26.28%\n",
      "26.63%\n",
      "26.98%\n",
      "27.33%\n",
      "27.68%\n",
      "28.03%\n",
      "28.38%\n",
      "28.73%\n",
      "29.08%\n",
      "29.43%\n",
      "29.78%\n",
      "30.13%\n",
      "30.48%\n",
      "30.83%\n",
      "31.19%\n",
      "31.54%\n",
      "31.89%\n",
      "32.24%\n",
      "32.59%\n",
      "32.94%\n",
      "33.29%\n",
      "33.64%\n",
      "33.99%\n",
      "34.34%\n",
      "34.69%\n",
      "35.04%\n",
      "35.39%\n",
      "35.74%\n",
      "36.09%\n",
      "36.44%\n",
      "36.79%\n",
      "37.14%\n",
      "37.49%\n",
      "37.84%\n",
      "38.19%\n",
      "38.54%\n",
      "38.89%\n",
      "39.24%\n",
      "39.59%\n",
      "39.95%\n",
      "40.30%\n",
      "40.65%\n",
      "41.00%\n",
      "41.35%\n",
      "41.70%\n",
      "42.05%\n",
      "42.40%\n",
      "42.75%\n",
      "43.10%\n",
      "43.45%\n",
      "43.80%\n",
      "44.15%\n",
      "44.50%\n",
      "44.85%\n",
      "45.20%\n",
      "45.55%\n",
      "45.90%\n",
      "46.25%\n",
      "46.60%\n",
      "46.95%\n",
      "47.30%\n",
      "47.65%\n",
      "48.00%\n",
      "48.35%\n",
      "48.71%\n",
      "49.06%\n",
      "49.41%\n",
      "49.76%\n",
      "50.11%\n",
      "50.46%\n",
      "50.81%\n",
      "51.16%\n",
      "51.51%\n",
      "51.86%\n",
      "52.21%\n",
      "52.56%\n",
      "52.91%\n",
      "53.26%\n",
      "53.61%\n",
      "53.96%\n",
      "54.31%\n",
      "54.66%\n",
      "55.01%\n",
      "55.36%\n",
      "55.71%\n",
      "56.06%\n",
      "56.41%\n",
      "56.76%\n",
      "57.11%\n",
      "57.47%\n",
      "57.82%\n",
      "58.17%\n",
      "58.52%\n",
      "58.87%\n",
      "59.22%\n",
      "59.57%\n",
      "59.92%\n",
      "60.27%\n",
      "60.62%\n",
      "60.97%\n",
      "61.32%\n",
      "61.67%\n",
      "62.02%\n",
      "62.37%\n",
      "62.72%\n",
      "63.07%\n",
      "63.42%\n",
      "63.77%\n",
      "64.12%\n",
      "64.47%\n",
      "64.82%\n",
      "65.17%\n",
      "65.52%\n",
      "65.87%\n",
      "66.23%\n",
      "66.58%\n",
      "66.93%\n",
      "67.28%\n",
      "67.63%\n",
      "67.98%\n",
      "68.33%\n",
      "68.68%\n",
      "69.03%\n",
      "69.38%\n",
      "69.73%\n",
      "70.08%\n",
      "70.43%\n",
      "70.78%\n",
      "71.13%\n",
      "71.48%\n",
      "71.83%\n",
      "72.18%\n",
      "72.53%\n",
      "72.88%\n",
      "73.23%\n",
      "73.58%\n",
      "73.93%\n",
      "74.28%\n",
      "74.63%\n",
      "74.99%\n",
      "75.34%\n",
      "75.69%\n",
      "76.04%\n",
      "76.39%\n",
      "76.74%\n",
      "77.09%\n",
      "77.44%\n",
      "77.79%\n",
      "78.14%\n",
      "78.49%\n",
      "78.84%\n",
      "79.19%\n",
      "79.54%\n",
      "79.89%\n",
      "80.24%\n",
      "80.59%\n",
      "80.94%\n",
      "81.29%\n",
      "81.64%\n",
      "81.99%\n",
      "82.34%\n",
      "82.69%\n",
      "83.04%\n",
      "83.39%\n",
      "83.75%\n",
      "84.10%\n",
      "84.45%\n",
      "84.80%\n",
      "85.15%\n",
      "85.50%\n",
      "85.85%\n",
      "86.20%\n",
      "86.55%\n",
      "86.90%\n",
      "87.25%\n",
      "87.60%\n",
      "87.95%\n",
      "88.30%\n",
      "88.65%\n",
      "89.00%\n",
      "89.35%\n",
      "89.70%\n",
      "90.05%\n",
      "90.40%\n",
      "90.75%\n",
      "91.10%\n",
      "91.45%\n",
      "91.80%\n",
      "92.15%\n",
      "92.50%\n",
      "92.86%\n",
      "93.21%\n",
      "93.56%\n",
      "93.91%\n",
      "94.26%\n",
      "94.61%\n",
      "94.96%\n",
      "95.31%\n",
      "95.66%\n",
      "96.01%\n",
      "96.36%\n",
      "96.71%\n",
      "97.06%\n",
      "97.41%\n",
      "97.76%\n",
      "98.11%\n",
      "98.46%\n",
      "98.81%\n",
      "99.16%\n",
      "99.51%\n",
      "99.86%\n"
     ]
    }
   ],
   "source": [
    "# quantization of librispeech data\n",
    "librispeech_train_dataset = LibriSpeechDataset(\n",
    "    root=\"./datasets\",\n",
    "    split=\"train\"\n",
    ")\n",
    "quantizer = Quantizer(DEVICE)\n",
    "quantized_indices_memory = {}\n",
    "for idx in range(len(librispeech_train_dataset)):\n",
    "    # show progress\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"{idx / len(librispeech_train_dataset) * 100:.2f}%\")\n",
    "    audio = librispeech_train_dataset[idx][-2].unsqueeze(0).to(DEVICE)\n",
    "    quantized_indices = quantizer.quantize(audio)\n",
    "    quantized_indices_memory[idx] = quantized_indices[0].tolist()\n",
    "with open(\"librispeech_quantized_indices_memory.pkl\", \"wb\") as f:\n",
    "    pickle.dump(quantized_indices_memory, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AND HIS BEST SUIT OF CLOTHES WHICH WAS PLAIN PROOF THAT HE WAS GOING OUT OF AVONLEA AND HE HAD THE BUGGY AND THE SORREL MARE WHICH BETOKENED THAT HE WAS GOING A CONSIDERABLE DISTANCE NOW WHERE WAS MATTHEW CUTHBERT GOING AND WHY WAS HE GOING THERE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librispeech_train_dataset[11][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataset = TIMITDatasetMelSpecdb(\"test\")\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False, drop_last=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quantized_indices_memory.pkl\", \"rb\") as f:\n",
    "    quantized_indices_memory = pickle.load(f)\n",
    "len(quantized_indices_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantizer = Quantizer(DEVICE)\n",
    "#quantized_indices_memory = {}\n",
    "#for idx in range(len(train_dataset)):\n",
    "#    # show progress\n",
    "#    if idx % 100 == 0:\n",
    "#        print(f\"{idx / len(train_dataset) * 100:.2f}%\")\n",
    "#    audio = train_dataset[idx][-1].unsqueeze(0).to(DEVICE)\n",
    "#    quantized_indices = quantizer.quantize(audio)\n",
    "#    quantized_indices_memory[idx] = quantized_indices[0].tolist()\n",
    "#with open(\"quantized_indices_memory.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(quantized_indices_memory, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_divergence_between_uniform_and_empirical_distribution(sampled_quantized_idx_count: torch.Tensor):\n",
    "    assert sampled_quantized_idx_count.ndim == 1\n",
    "    # update sampled_quantized_idx_count\n",
    "    # calculate KL divergence\n",
    "    empirical_distribution = sampled_quantized_idx_count / sampled_quantized_idx_count.sum()\n",
    "    uniform_distribution = torch.ones_like(empirical_distribution) / len(empirical_distribution)\n",
    "    kl_divergence = torch.sum(empirical_distribution * torch.log(empirical_distribution / uniform_distribution))\n",
    "    if kl_divergence < 0:\n",
    "        raise ValueError(\"KL divergence must be positive.\")\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_quantized_idx_count = torch.randint(0, 200, (320*320,), dtype=torch.float32)\n",
    "sampled_quantized_idx_count += 1e-8\n",
    "calculate_kl_divergence_between_uniform_and_empirical_distribution(sampled_quantized_idx_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLBasedSampler:\n",
    "    def __init__(self, quantized_indices_memory: Dict, dataset: torch.utils.data.Dataset, ratio: float, device: torch.device):\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.ratio = ratio\n",
    "        self.target_num_samples = int(len(dataset) * ratio)\n",
    "        self.initial_num_samples =int(0.1 * self.target_num_samples)\n",
    "        self.sampled_indices = set()\n",
    "        self.not_sampled_indices = set(range(len(dataset)))\n",
    "        self.sampled_quantized_idx_count = torch.zeros(320 * 320, dtype=torch.float32)\n",
    "        self.quantized_indices_memory = quantized_indices_memory\n",
    "        # sample initial samples\n",
    "        self.sample_initial_samples()\n",
    "    \n",
    "    def calculate_kl_divergence_between_uniform_and_empirical_distribution(self, sample):\n",
    "        \"\"\"Calculate KL divergence between uniform and empirical distribution.\n",
    "        Args:\n",
    "            sample (1D torch.Tensor): quantized indices tensor\n",
    "        Returns:\n",
    "            kl_divergence (float): KL divergence between uniform and empirical distribution.\n",
    "        \"\"\"\n",
    "        assert sample.ndim == 1\n",
    "        # update sampled_quantized_idx_count\n",
    "        sampled_quantized_idx_count = self.sampled_quantized_idx_count.clone()\n",
    "        with torch.no_grad():\n",
    "            for quantized_idx in sample:\n",
    "                sampled_quantized_idx_count[quantized_idx] += 1\n",
    "        # calculate KL divergence\n",
    "        # avoid zero division\n",
    "        sampled_quantized_idx_count += 1e-8\n",
    "        empirical_distribution = sampled_quantized_idx_count / sampled_quantized_idx_count.sum()\n",
    "        uniform_distribution = torch.ones_like(empirical_distribution) / len(empirical_distribution)\n",
    "        kl_divergence = torch.sum(empirical_distribution * torch.log(empirical_distribution / uniform_distribution))\n",
    "        if kl_divergence < 0:\n",
    "            raise ValueError(\"KL divergence must be positive.\")\n",
    "        return kl_divergence\n",
    "\n",
    "    def sample_initial_samples(self):\n",
    "        # sample initial samples based on random sampling\n",
    "        initial_indices = torch.randperm(len(self.dataset))[:self.initial_num_samples].tolist()\n",
    "        self.sampled_indices.update(initial_indices)\n",
    "        self.not_sampled_indices.difference_update(initial_indices)\n",
    "        # update sampled_quantized_idx_count\n",
    "        with torch.no_grad():\n",
    "            for idx in initial_indices:\n",
    "                quantized_indices = torch.tensor(self.quantized_indices_memory[idx])\n",
    "                for quantized_idx in quantized_indices:\n",
    "                    self.sampled_quantized_idx_count[quantized_idx] += 1\n",
    "\n",
    "    def sample(self):\n",
    "        # sample new samples based on KL divergence until the number of samples reaches the target number of samples\n",
    "        # return subset of dataset\n",
    "        while len(self.sampled_indices) < self.target_num_samples:\n",
    "            # show progress\n",
    "            print(f\"sampled {len(self.sampled_indices) / self.target_num_samples * 100:.2f} %\")\n",
    "            # calculate KL divergence for each not sampled indices\n",
    "            kl_divergences = {}\n",
    "            not_sampled_indices = list(self.not_sampled_indices)\n",
    "            for idx in not_sampled_indices:\n",
    "                quantized_indices = torch.tensor(self.quantized_indices_memory[idx])\n",
    "                kl_divergence = self.calculate_kl_divergence_between_uniform_and_empirical_distribution(quantized_indices)\n",
    "                kl_divergences[idx] = kl_divergence\n",
    "            # select the index with the minimum KL divergence\n",
    "            min_kl_divergence_idx = min(kl_divergences.keys(), key=kl_divergences.get)\n",
    "            print(f\"min_kl_divergence: {kl_divergences[min_kl_divergence_idx]}\")\n",
    "            # update sampled_indices and not_sampled_indices\n",
    "            self.sampled_indices.add(min_kl_divergence_idx)\n",
    "            self.not_sampled_indices.remove(min_kl_divergence_idx)\n",
    "            # update sampled_quantized_idx_count\n",
    "            quantized_indices = self.quantized_indices_memory[min_kl_divergence_idx]\n",
    "            for quantized_idx in quantized_indices:\n",
    "                self.sampled_quantized_idx_count[quantized_idx] += 1\n",
    "\n",
    "        # return subset of self.dataset\n",
    "        return torch.utils.data.Subset(self.dataset, list(self.sampled_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TIMITDatasetMelSpecdb(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_based_sampler = KLBasedSampler(quantized_indices_memory=quantized_indices_memory, dataset=train_dataset, ratio=0.1, device=DEVICE)\n",
    "subset = kl_based_sampler.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save subset\n",
    "with open(\"subset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampler(dataset: torch.utils.data.Dataset, ratio: float):\n",
    "    \"\"\"Randomly sample a subset of the dataset\"\"\"\n",
    "    num_samples = int(len(dataset) * ratio)\n",
    "    indices = torch.randperm(len(dataset))[:num_samples].tolist()\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_train_subset = random_sampler(train_dataset, 0.8)\n",
    "random_train_dataloader = get_dataloader(random_train_subset, batch_size=32, shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"This Learning is running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = 80\n",
    "self_attn_dim = 256\n",
    "feed_forward_dim = 1024\n",
    "token_size = len(train_dataset.vocab.keys())\n",
    "num_epochs = 40\n",
    "total_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing.subsampling import Conv2DSubSampling\n",
    "from modules.transformers.encoder import TransformerEncoder\n",
    "from modules.transformers.scheduler import TransformerLR\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, nlabel):\n",
    "        super(Model, self).__init__()\n",
    "        self.nlabel = nlabel\n",
    "        # out_sizeがSelf-Attentionの入力次元\n",
    "        self.conv2d_sub_sampling = Conv2DSubSampling(\n",
    "            in_size=80, out_size=256, kernel_size1=3, kernel_size2=3, stride1=2, stride2=1)\n",
    "        # in_hidden_sizeがFFの次元\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            in_size=256, nlayer=12, nhead=4, in_hidden_size=1024, dropout=0.1, norm_first=True)\n",
    "        self.fc = nn.Linear(256, nlabel, bias=True)\n",
    "        self.log_softmax = nn.functional.log_softmax\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        # args:\n",
    "        #   x: [B, T, in_size]\n",
    "        #   x_lengths: [B]\n",
    "        #       padding前のシーケンス長\n",
    "        # return:\n",
    "        #   log_prob: [B, T, nlabel]\n",
    "        #   y_lengths: [B]\n",
    "        #       非パディング部分のシーケンス長\n",
    "        subsampled_x, subsampled_x_length = self.conv2d_sub_sampling(x, x_lengths)\n",
    "        encoded, encoded_inner = self.transformer_encoder(\n",
    "            subsampled_x, subsampled_x_length)  # [B, T', subsampled_in_size]\n",
    "        y = self.fc(encoded)  # [B, T', nlabel]\n",
    "        y_lengths = subsampled_x_length\n",
    "        log_probs = self.log_softmax(y, dim=-1)  # [B, T', nlabel]\n",
    "        return log_probs, y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "asr_model = Model(nlabel=token_size).to(DEVICE)\n",
    "ctc_loss = torch.nn.CTCLoss(reduction=\"sum\", blank=train_dataset.ctc_token_id)\n",
    "optimizer = torch.optim.Adam(asr_model.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = TransformerLR(optimizer, d_model=256, warmup_steps=8000) # Warmup終了時点でおよそ0.0017になっている\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    asr_model.train()\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_cer = 0\n",
    "    train_epoch_wer = 0\n",
    "    train_cnt = 0\n",
    "\n",
    "    for i, (bidx, bx, bx_len, by, by_len, texts, phonetic_details) in enumerate(random_train_dataloader):\n",
    "        bx = bx.to(DEVICE)\n",
    "        bx_len = bx_len.to(DEVICE)\n",
    "        by = by.to(DEVICE)\n",
    "        by_len = by_len.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        log_probs, y_lengths = asr_model(bx, bx_len)\n",
    "        loss = ctc_loss(log_probs.transpose(1, 0), by, y_lengths, by_len)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_epoch_loss += (loss.item() / bx.size(0))\n",
    "\n",
    "        # calculate CER\n",
    "        hypothesis = torch.argmax(log_probs, dim=-1)\n",
    "        hypotheses = greedy_decoder(hypothesis, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "        answers = greedy_decoder(by, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "        train_epoch_cer += char_error_rate(hypotheses, answers)\n",
    "        train_epoch_wer += word_error_rate(hypotheses, answers)\n",
    "\n",
    "        train_cnt += 1\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs} train loss: {train_epoch_loss / train_cnt}, train CER: {train_epoch_cer / train_cnt}, train WER: {train_epoch_wer / train_cnt}\")\n",
    "\n",
    "    asr_model.eval()\n",
    "    test_epoch_loss = 0\n",
    "    test_epoch_cer = 0\n",
    "    test_epoch_wer = 0\n",
    "    test_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (bidx, bx, bx_len, by, by_len, texts, phonetic_details) in enumerate(test_dataloader):\n",
    "            bx = bx.to(DEVICE)\n",
    "            bx_len = bx_len.to(DEVICE)\n",
    "            by = by.to(DEVICE)\n",
    "            by_len = by_len.to(DEVICE)\n",
    "            log_probs, y_lengths = asr_model(bx, bx_len)\n",
    "            loss = ctc_loss(log_probs.transpose(1, 0), by, y_lengths, by_len)\n",
    "            test_epoch_loss += (loss.item() / bx.size(0))\n",
    "\n",
    "            # calculate CER\n",
    "            hypothesis = torch.argmax(log_probs, dim=-1)\n",
    "            hypotheses = greedy_decoder(hypothesis, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "            answers = greedy_decoder(by, train_dataset.vocab, \"[PAD]\", \"|\", \"_\")\n",
    "            test_epoch_cer += char_error_rate(hypotheses, answers)\n",
    "            test_epoch_wer += word_error_rate(hypotheses, answers)\n",
    "\n",
    "            test_cnt += 1\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} of {num_epochs} test loss: {test_epoch_loss / test_cnt}, test CER: {test_epoch_cer / test_cnt}, test WER: {test_epoch_wer / test_cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e822874ab5bf40d8e254332eebb695e7fa04bbc22c17addc03c9268ee429b8b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
